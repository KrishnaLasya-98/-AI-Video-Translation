{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b211b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing s10_processed: 100%|██████████| 1000/1000 [11:51<00:00,  1.41it/s]\n",
      "Processing s11_processed: 100%|██████████| 1000/1000 [10:17<00:00,  1.62it/s]\n",
      "Processing s12_processed: 100%|██████████| 1000/1000 [10:04<00:00,  1.65it/s]\n",
      "Processing s13_processed: 100%|██████████| 1000/1000 [11:15<00:00,  1.48it/s]\n",
      "Processing s14_processed: 100%|██████████| 1000/1000 [11:53<00:00,  1.40it/s]\n",
      "Processing s15_processed: 100%|██████████| 1000/1000 [10:32<00:00,  1.58it/s]\n",
      "Processing s16_processed: 100%|██████████| 1000/1000 [09:44<00:00,  1.71it/s]\n",
      "Processing s17_processed: 100%|██████████| 1000/1000 [11:55<00:00,  1.40it/s]\n",
      "Processing s18_processed: 100%|██████████| 1000/1000 [09:48<00:00,  1.70it/s]\n",
      "Processing s19_processed: 100%|██████████| 1000/1000 [09:39<00:00,  1.73it/s]\n",
      "Processing s1_processed: 100%|██████████| 1000/1000 [11:55<00:00,  1.40it/s]\n",
      "Processing s20_processed: 100%|██████████| 1000/1000 [11:43<00:00,  1.42it/s]\n",
      " 36%|███▋      | 12/33 [2:10:41<3:53:02, 665.83s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define paths - replace these with your actual paths\n",
    "DATA_ROOT = \"E:\\\\training data\"\n",
    "OUTPUT_DIR = \"E:\\\\play_6.0\\\\processed_data\"\n",
    "\n",
    "def extract_frames_and_audio(video_path, frames_dir, audio_path):\n",
    "    \"\"\"Extract frames and audio from video file\"\"\"\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract frames\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frame_idx = 0\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        cv2.imwrite(os.path.join(frames_dir, f'{frame_idx}.jpg'), frame)\n",
    "        frame_idx += 1\n",
    "    video.release()\n",
    "    \n",
    "    # Extract audio\n",
    "    command = f'ffmpeg -y -i \"{video_path}\" -strict -2 \"{audio_path}\"'\n",
    "    subprocess.call(command, shell=True)\n",
    "\n",
    "def process_dataset(data_root, output_dir):\n",
    "    \"\"\"Process all videos in the dataset\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create filelists directory\n",
    "    filelists_dir = os.path.join(output_dir, 'filelists')\n",
    "    os.makedirs(filelists_dir, exist_ok=True)\n",
    "    \n",
    "    train_filelist = []\n",
    "    val_filelist = []\n",
    "    \n",
    "    # Process each subject folder\n",
    "    subject_dirs = [d for d in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, d)) and d.endswith('_processed')]\n",
    "    \n",
    "    for subject_dir in tqdm(subject_dirs):\n",
    "        subject_path = os.path.join(data_root, subject_dir)\n",
    "        \n",
    "        # Get all video files\n",
    "        video_files = [f for f in os.listdir(subject_path) if f.endswith('.mpg')]\n",
    "        \n",
    "        # Process each video\n",
    "        for video_file in tqdm(video_files, desc=f'Processing {subject_dir}'):\n",
    "            video_name = os.path.splitext(video_file)[0]\n",
    "            video_path = os.path.join(subject_path, video_file)\n",
    "            \n",
    "            # Create output directories\n",
    "            video_output_dir = os.path.join(output_dir, subject_dir, video_name)\n",
    "            frames_dir = os.path.join(video_output_dir)\n",
    "            audio_path = os.path.join(video_output_dir, 'audio.wav')\n",
    "            \n",
    "            # Extract frames and audio\n",
    "            extract_frames_and_audio(video_path, frames_dir, audio_path)\n",
    "            \n",
    "            # Copy alignment file if it exists\n",
    "            align_file = os.path.join(subject_path, 'align', f'{video_name}.align')\n",
    "            if os.path.exists(align_file):\n",
    "                os.makedirs(os.path.join(video_output_dir, 'align'), exist_ok=True)\n",
    "                with open(align_file, 'r') as src, open(os.path.join(video_output_dir, 'align', f'{video_name}.align'), 'w') as dst:\n",
    "                    dst.write(src.read())\n",
    "            \n",
    "            # Add to filelist (80% train, 20% val)\n",
    "            if np.random.rand() < 0.8:\n",
    "                train_filelist.append(os.path.join(subject_dir, video_name))\n",
    "            else:\n",
    "                val_filelist.append(os.path.join(subject_dir, video_name))\n",
    "    \n",
    "    # Write filelists\n",
    "    with open(os.path.join(filelists_dir, 'train.txt'), 'w') as f:\n",
    "        f.write('\\n'.join(train_filelist))\n",
    "    \n",
    "    with open(os.path.join(filelists_dir, 'val.txt'), 'w') as f:\n",
    "        f.write('\\n'.join(val_filelist))\n",
    "\n",
    "# Run the preprocessing\n",
    "process_dataset(DATA_ROOT, OUTPUT_DIR)\n",
    "print(\"Preprocessing completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad727ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils import data as data_utils\n",
    "import numpy as np\n",
    "import argparse\n",
    "import sys\n",
    "sys.path.append('E:\\\\play_6.0\\\\Wav2Lip')\n",
    "\n",
    "# Import Wav2Lip modules\n",
    "from hparams import hparams\n",
    "from models import SyncNet_color as SyncNet\n",
    "from models import Wav2Lip as Wav2Lip\n",
    "from models import Wav2Lip_disc_qual as Discriminator\n",
    "\n",
    "# Define paths\n",
    "DATA_ROOT = \"E:\\\\play_6.0\\\\processed_data\"\n",
    "CHECKPOINT_DIR = \"E:\\\\play_6.0\\\\checkpoints\"\n",
    "SYNCNET_PATH = \"E:\\\\play_6.0\\\\Wav2Lip\\\\checkpoints\\\\syncnet.pth\"\n",
    "\n",
    "# Set improved hyperparameters\n",
    "hparams.set_hparam('batch_size', 8)  # Smaller batch size for better quality\n",
    "hparams.set_hparam('syncnet_wt', 0.03)  # Start with higher sync weight\n",
    "hparams.set_hparam('disc_wt', 0.1)  # Increase discriminator weight for better visual quality\n",
    "hparams.set_hparam('img_size', 128)  # Increase image size for better resolution\n",
    "hparams.set_hparam('initial_learning_rate', 5e-5)  # Lower learning rate for more stable training\n",
    "\n",
    "# Run the training command\n",
    "command = f'python E:\\\\play_6.0\\\\Wav2Lip\\\\wav2lip_train.py --data_root {DATA_ROOT} --checkpoint_dir {CHECKPOINT_DIR} --syncnet_checkpoint_path {SYNCNET_PATH}'\n",
    "os.system(command)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c70f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils import data as data_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os, random, cv2, argparse\n",
    "from hparams import hparams, get_image_list\n",
    "\n",
    "from models import SyncNet_color as SyncNet\n",
    "from models import Wav2Lip as Wav2Lip\n",
    "from models import Wav2Lip_disc_qual as Discriminator\n",
    "import audio\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Code to train the Wav2Lip model with the visual quality discriminator')\n",
    "parser.add_argument(\"--data_root\", help=\"Root folder of the preprocessed dataset\", required=True)\n",
    "parser.add_argument('--checkpoint_dir', help='Save checkpoints to this directory', required=True)\n",
    "parser.add_argument('--syncnet_checkpoint_path', help='Load the pre-trained Expert discriminator', required=True)\n",
    "parser.add_argument('--checkpoint_path', help='Resume from this checkpoint', default=None)\n",
    "parser.add_argument('--disc_checkpoint_path', help='Resume discriminator from this checkpoint', default=None)\n",
    "args = parser.parse_args()\n",
    "\n",
    "global_step = 0\n",
    "global_epoch = 0\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('use_cuda: {}'.format(use_cuda))\n",
    "\n",
    "syncnet_T = 5\n",
    "syncnet_mel_step_size = 16\n",
    "\n",
    "# Rest of the code from wav2lip_train.py with modifications for GAN training\n",
    "\n",
    "# Add discriminator loss\n",
    "def train(device, model, disc_model, train_data_loader, test_data_loader, optimizer, disc_optimizer,\n",
    "          checkpoint_dir=None, checkpoint_interval=None, nepochs=None):\n",
    "    global global_step, global_epoch\n",
    "    resumed_step = global_step\n",
    " \n",
    "    while global_epoch < nepochs:\n",
    "        print('Starting Epoch: {}'.format(global_epoch))\n",
    "        running_sync_loss, running_l1_loss, running_disc_loss, running_perceptual_loss = 0., 0., 0., 0.\n",
    "        prog_bar = tqdm(enumerate(train_data_loader))\n",
    "        for step, (x, indiv_mels, mel, gt) in prog_bar:\n",
    "            model.train()\n",
    "            disc_model.train()\n",
    "            optimizer.zero_grad()\n",
    "            disc_optimizer.zero_grad()\n",
    "\n",
    "            # Move data to CUDA device\n",
    "            x = x.to(device)\n",
    "            mel = mel.to(device)\n",
    "            indiv_mels = indiv_mels.to(device)\n",
    "            gt = gt.to(device)\n",
    "\n",
    "            g = model(indiv_mels, x)\n",
    "\n",
    "            # Sync loss\n",
    "            if hparams.syncnet_wt > 0.:\n",
    "                sync_loss = get_sync_loss(mel, g)\n",
    "            else:\n",
    "                sync_loss = 0.\n",
    "\n",
    "            # L1 loss\n",
    "            l1loss = recon_loss(g, gt)\n",
    "\n",
    "            # Perceptual loss (GAN)\n",
    "            if hparams.disc_wt > 0.:\n",
    "                perceptual_loss = disc_model.perceptual_forward(g)\n",
    "            else:\n",
    "                perceptual_loss = 0.\n",
    "\n",
    "            # Combined loss\n",
    "            loss = hparams.syncnet_wt * sync_loss + (1 - hparams.syncnet_wt - hparams.disc_wt) * l1loss\n",
    "            if hparams.disc_wt > 0.:\n",
    "                loss += hparams.disc_wt * perceptual_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Train discriminator\n",
    "            if hparams.disc_wt > 0.:\n",
    "                disc_optimizer.zero_grad()\n",
    "                \n",
    "                # Real samples - should be classified as real (1)\n",
    "                real_pred = disc_model(gt)\n",
    "                real_loss = F.binary_cross_entropy(real_pred, torch.ones((len(real_pred), 1)).to(device))\n",
    "                \n",
    "                # Fake samples - should be classified as fake (0)\n",
    "                fake_pred = disc_model(g.detach())\n",
    "                fake_loss = F.binary_cross_entropy(fake_pred, torch.zeros((len(fake_pred), 1)).to(device))\n",
    "                \n",
    "                disc_loss = (real_loss + fake_loss) / 2\n",
    "                disc_loss.backward()\n",
    "                disc_optimizer.step()\n",
    "            else:\n",
    "                disc_loss = 0.\n",
    "\n",
    "            # Rest of the training loop...\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    checkpoint_dir = args.checkpoint_dir\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Dataset and Dataloader setup\n",
    "    train_dataset = Dataset('train')\n",
    "    test_dataset = Dataset('val')\n",
    "\n",
    "    train_data_loader = data_utils.DataLoader(\n",
    "        train_dataset, batch_size=hparams.batch_size, shuffle=True,\n",
    "        num_workers=hparams.num_workers)\n",
    "\n",
    "    test_data_loader = data_utils.DataLoader(\n",
    "        test_dataset, batch_size=hparams.batch_size,\n",
    "        num_workers=4)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    # Model\n",
    "    model = Wav2Lip().to(device)\n",
    "    disc_model = Discriminator().to(device)\n",
    "    \n",
    "    print('Generator: total trainable params {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "    print('Discriminator: total trainable params {}'.format(sum(p.numel() for p in disc_model.parameters() if p.requires_grad)))\n",
    "\n",
    "    optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                           lr=hparams.initial_learning_rate)\n",
    "    disc_optimizer = optim.Adam([p for p in disc_model.parameters() if p.requires_grad],\n",
    "                           lr=hparams.disc_initial_learning_rate)\n",
    "\n",
    "    if args.checkpoint_path is not None:\n",
    "        load_checkpoint(args.checkpoint_path, model, optimizer, reset_optimizer=False)\n",
    "        \n",
    "    if args.disc_checkpoint_path is not None:\n",
    "        load_checkpoint(args.disc_checkpoint_path, disc_model, disc_optimizer, reset_optimizer=False, overwrite_global_states=False)\n",
    "        \n",
    "    load_checkpoint(args.syncnet_checkpoint_path, syncnet, None, reset_optimizer=True, overwrite_global_states=False)\n",
    "\n",
    "    # Train!\n",
    "    train(device, model, disc_model, train_data_loader, test_data_loader, optimizer, disc_optimizer,\n",
    "              checkpoint_dir=checkpoint_dir,\n",
    "              checkpoint_interval=hparams.checkpoint_interval,\n",
    "              nepochs=hparams.nepochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d2cd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils import data as data_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os, random, cv2, argparse\n",
    "from hparams import hparams, get_image_list\n",
    "\n",
    "from models import SyncNet_color as SyncNet\n",
    "from models import Wav2Lip as Wav2Lip\n",
    "from models import Wav2Lip_disc_qual as Discriminator\n",
    "import audio\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Code to train the Wav2Lip model with the visual quality discriminator')\n",
    "parser.add_argument(\"--data_root\", help=\"Root folder of the preprocessed dataset\", required=True)\n",
    "parser.add_argument('--checkpoint_dir', help='Save checkpoints to this directory', required=True)\n",
    "parser.add_argument('--syncnet_checkpoint_path', help='Load the pre-trained Expert discriminator', required=True)\n",
    "parser.add_argument('--checkpoint_path', help='Resume from this checkpoint', default=None)\n",
    "parser.add_argument('--disc_checkpoint_path', help='Resume discriminator from this checkpoint', default=None)\n",
    "args = parser.parse_args()\n",
    "\n",
    "global_step = 0\n",
    "global_epoch = 0\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('use_cuda: {}'.format(use_cuda))\n",
    "\n",
    "syncnet_T = 5\n",
    "syncnet_mel_step_size = 16\n",
    "\n",
    "# Rest of the code from wav2lip_train.py with modifications for GAN training\n",
    "\n",
    "# Add discriminator loss\n",
    "def train(device, model, disc_model, train_data_loader, test_data_loader, optimizer, disc_optimizer,\n",
    "          checkpoint_dir=None, checkpoint_interval=None, nepochs=None):\n",
    "    global global_step, global_epoch\n",
    "    resumed_step = global_step\n",
    " \n",
    "    while global_epoch < nepochs:\n",
    "        print('Starting Epoch: {}'.format(global_epoch))\n",
    "        running_sync_loss, running_l1_loss, running_disc_loss, running_perceptual_loss = 0., 0., 0., 0.\n",
    "        prog_bar = tqdm(enumerate(train_data_loader))\n",
    "        for step, (x, indiv_mels, mel, gt) in prog_bar:\n",
    "            model.train()\n",
    "            disc_model.train()\n",
    "            optimizer.zero_grad()\n",
    "            disc_optimizer.zero_grad()\n",
    "\n",
    "            # Move data to CUDA device\n",
    "            x = x.to(device)\n",
    "            mel = mel.to(device)\n",
    "            indiv_mels = indiv_mels.to(device)\n",
    "            gt = gt.to(device)\n",
    "\n",
    "            g = model(indiv_mels, x)\n",
    "\n",
    "            # Sync loss\n",
    "            if hparams.syncnet_wt > 0.:\n",
    "                sync_loss = get_sync_loss(mel, g)\n",
    "            else:\n",
    "                sync_loss = 0.\n",
    "\n",
    "            # L1 loss\n",
    "            l1loss = recon_loss(g, gt)\n",
    "\n",
    "            # Perceptual loss (GAN)\n",
    "            if hparams.disc_wt > 0.:\n",
    "                perceptual_loss = disc_model.perceptual_forward(g)\n",
    "            else:\n",
    "                perceptual_loss = 0.\n",
    "\n",
    "            # Combined loss\n",
    "            loss = hparams.syncnet_wt * sync_loss + (1 - hparams.syncnet_wt - hparams.disc_wt) * l1loss\n",
    "            if hparams.disc_wt > 0.:\n",
    "                loss += hparams.disc_wt * perceptual_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Train discriminator\n",
    "            if hparams.disc_wt > 0.:\n",
    "                disc_optimizer.zero_grad()\n",
    "                \n",
    "                # Real samples - should be classified as real (1)\n",
    "                real_pred = disc_model(gt)\n",
    "                real_loss = F.binary_cross_entropy(real_pred, torch.ones((len(real_pred), 1)).to(device))\n",
    "                \n",
    "                # Fake samples - should be classified as fake (0)\n",
    "                fake_pred = disc_model(g.detach())\n",
    "                fake_loss = F.binary_cross_entropy(fake_pred, torch.zeros((len(fake_pred), 1)).to(device))\n",
    "                \n",
    "                disc_loss = (real_loss + fake_loss) / 2\n",
    "                disc_loss.backward()\n",
    "                disc_optimizer.step()\n",
    "            else:\n",
    "                disc_loss = 0.\n",
    "\n",
    "            # Rest of the training loop...\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    checkpoint_dir = args.checkpoint_dir\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Dataset and Dataloader setup\n",
    "    train_dataset = Dataset('train')\n",
    "    test_dataset = Dataset('val')\n",
    "\n",
    "    train_data_loader = data_utils.DataLoader(\n",
    "        train_dataset, batch_size=hparams.batch_size, shuffle=True,\n",
    "        num_workers=hparams.num_workers)\n",
    "\n",
    "    test_data_loader = data_utils.DataLoader(\n",
    "        test_dataset, batch_size=hparams.batch_size,\n",
    "        num_workers=4)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    # Model\n",
    "    model = Wav2Lip().to(device)\n",
    "    disc_model = Discriminator().to(device)\n",
    "    \n",
    "    print('Generator: total trainable params {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "    print('Discriminator: total trainable params {}'.format(sum(p.numel() for p in disc_model.parameters() if p.requires_grad)))\n",
    "\n",
    "    optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                           lr=hparams.initial_learning_rate)\n",
    "    disc_optimizer = optim.Adam([p for p in disc_model.parameters() if p.requires_grad],\n",
    "                           lr=hparams.disc_initial_learning_rate)\n",
    "\n",
    "    if args.checkpoint_path is not None:\n",
    "        load_checkpoint(args.checkpoint_path, model, optimizer, reset_optimizer=False)\n",
    "        \n",
    "    if args.disc_checkpoint_path is not None:\n",
    "        load_checkpoint(args.disc_checkpoint_path, disc_model, disc_optimizer, reset_optimizer=False, overwrite_global_states=False)\n",
    "        \n",
    "    load_checkpoint(args.syncnet_checkpoint_path, syncnet, None, reset_optimizer=True, overwrite_global_states=False)\n",
    "\n",
    "    # Train!\n",
    "    train(device, model, disc_model, train_data_loader, test_data_loader, optimizer, disc_optimizer,\n",
    "              checkpoint_dir=checkpoint_dir,\n",
    "              checkpoint_interval=hparams.checkpoint_interval,\n",
    "              nepochs=hparams.nepochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02155de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils import data as data_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os, random, cv2, argparse\n",
    "from hparams import hparams, get_image_list\n",
    "\n",
    "from models import SyncNet_color as SyncNet\n",
    "from models import Wav2Lip as Wav2Lip\n",
    "from models import Wav2Lip_disc_qual as Discriminator\n",
    "import audio\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Code to train the Wav2Lip model with the visual quality discriminator')\n",
    "parser.add_argument(\"--data_root\", help=\"Root folder of the preprocessed dataset\", required=True)\n",
    "parser.add_argument('--checkpoint_dir', help='Save checkpoints to this directory', required=True)\n",
    "parser.add_argument('--syncnet_checkpoint_path', help='Load the pre-trained Expert discriminator', required=True)\n",
    "parser.add_argument('--checkpoint_path', help='Resume from this checkpoint', default=None)\n",
    "parser.add_argument('--disc_checkpoint_path', help='Resume discriminator from this checkpoint', default=None)\n",
    "args = parser.parse_args()\n",
    "\n",
    "global_step = 0\n",
    "global_epoch = 0\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('use_cuda: {}'.format(use_cuda))\n",
    "\n",
    "syncnet_T = 5\n",
    "syncnet_mel_step_size = 16\n",
    "\n",
    "# Rest of the code from wav2lip_train.py with modifications for GAN training\n",
    "\n",
    "# Add discriminator loss\n",
    "def train(device, model, disc_model, train_data_loader, test_data_loader, optimizer, disc_optimizer,\n",
    "          checkpoint_dir=None, checkpoint_interval=None, nepochs=None):\n",
    "    global global_step, global_epoch\n",
    "    resumed_step = global_step\n",
    " \n",
    "    while global_epoch < nepochs:\n",
    "        print('Starting Epoch: {}'.format(global_epoch))\n",
    "        running_sync_loss, running_l1_loss, running_disc_loss, running_perceptual_loss = 0., 0., 0., 0.\n",
    "        prog_bar = tqdm(enumerate(train_data_loader))\n",
    "        for step, (x, indiv_mels, mel, gt) in prog_bar:\n",
    "            model.train()\n",
    "            disc_model.train()\n",
    "            optimizer.zero_grad()\n",
    "            disc_optimizer.zero_grad()\n",
    "\n",
    "            # Move data to CUDA device\n",
    "            x = x.to(device)\n",
    "            mel = mel.to(device)\n",
    "            indiv_mels = indiv_mels.to(device)\n",
    "            gt = gt.to(device)\n",
    "\n",
    "            g = model(indiv_mels, x)\n",
    "\n",
    "            # Sync loss\n",
    "            if hparams.syncnet_wt > 0.:\n",
    "                sync_loss = get_sync_loss(mel, g)\n",
    "            else:\n",
    "                sync_loss = 0.\n",
    "\n",
    "            # L1 loss\n",
    "            l1loss = recon_loss(g, gt)\n",
    "\n",
    "            # Perceptual loss (GAN)\n",
    "            if hparams.disc_wt > 0.:\n",
    "                perceptual_loss = disc_model.perceptual_forward(g)\n",
    "            else:\n",
    "                perceptual_loss = 0.\n",
    "\n",
    "            # Combined loss\n",
    "            loss = hparams.syncnet_wt * sync_loss + (1 - hparams.syncnet_wt - hparams.disc_wt) * l1loss\n",
    "            if hparams.disc_wt > 0.:\n",
    "                loss += hparams.disc_wt * perceptual_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Train discriminator\n",
    "            if hparams.disc_wt > 0.:\n",
    "                disc_optimizer.zero_grad()\n",
    "                \n",
    "                # Real samples - should be classified as real (1)\n",
    "                real_pred = disc_model(gt)\n",
    "                real_loss = F.binary_cross_entropy(real_pred, torch.ones((len(real_pred), 1)).to(device))\n",
    "                \n",
    "                # Fake samples - should be classified as fake (0)\n",
    "                fake_pred = disc_model(g.detach())\n",
    "                fake_loss = F.binary_cross_entropy(fake_pred, torch.zeros((len(fake_pred), 1)).to(device))\n",
    "                \n",
    "                disc_loss = (real_loss + fake_loss) / 2\n",
    "                disc_loss.backward()\n",
    "                disc_optimizer.step()\n",
    "            else:\n",
    "                disc_loss = 0.\n",
    "\n",
    "            # Rest of the training loop...\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    checkpoint_dir = args.checkpoint_dir\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Dataset and Dataloader setup\n",
    "    train_dataset = Dataset('train')\n",
    "    test_dataset = Dataset('val')\n",
    "\n",
    "    train_data_loader = data_utils.DataLoader(\n",
    "        train_dataset, batch_size=hparams.batch_size, shuffle=True,\n",
    "        num_workers=hparams.num_workers)\n",
    "\n",
    "    test_data_loader = data_utils.DataLoader(\n",
    "        test_dataset, batch_size=hparams.batch_size,\n",
    "        num_workers=4)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    # Model\n",
    "    model = Wav2Lip().to(device)\n",
    "    disc_model = Discriminator().to(device)\n",
    "    \n",
    "    print('Generator: total trainable params {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "    print('Discriminator: total trainable params {}'.format(sum(p.numel() for p in disc_model.parameters() if p.requires_grad)))\n",
    "\n",
    "    optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                           lr=hparams.initial_learning_rate)\n",
    "    disc_optimizer = optim.Adam([p for p in disc_model.parameters() if p.requires_grad],\n",
    "                           lr=hparams.disc_initial_learning_rate)\n",
    "\n",
    "    if args.checkpoint_path is not None:\n",
    "        load_checkpoint(args.checkpoint_path, model, optimizer, reset_optimizer=False)\n",
    "        \n",
    "    if args.disc_checkpoint_path is not None:\n",
    "        load_checkpoint(args.disc_checkpoint_path, disc_model, disc_optimizer, reset_optimizer=False, overwrite_global_states=False)\n",
    "        \n",
    "    load_checkpoint(args.syncnet_checkpoint_path, syncnet, None, reset_optimizer=True, overwrite_global_states=False)\n",
    "\n",
    "    # Train!\n",
    "    train(device, model, disc_model, train_data_loader, test_data_loader, optimizer, disc_optimizer,\n",
    "              checkpoint_dir=checkpoint_dir,\n",
    "              checkpoint_interval=hparams.checkpoint_interval,\n",
    "              nepochs=hparams.nepochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f3990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils import data as data_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os, random, cv2, argparse\n",
    "from hparams import hparams, get_image_list\n",
    "\n",
    "from models import SyncNet_color as SyncNet\n",
    "from models import Wav2Lip as Wav2Lip\n",
    "from models import Wav2Lip_disc_qual as Discriminator\n",
    "import audio\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Code to train the Wav2Lip model with the visual quality discriminator')\n",
    "parser.add_argument(\"--data_root\", help=\"Root folder of the preprocessed dataset\", required=True)\n",
    "parser.add_argument('--checkpoint_dir', help='Save checkpoints to this directory', required=True)\n",
    "parser.add_argument('--syncnet_checkpoint_path', help='Load the pre-trained Expert discriminator', required=True)\n",
    "parser.add_argument('--checkpoint_path', help='Resume from this checkpoint', default=None)\n",
    "parser.add_argument('--disc_checkpoint_path', help='Resume discriminator from this checkpoint', default=None)\n",
    "args = parser.parse_args()\n",
    "\n",
    "global_step = 0\n",
    "global_epoch = 0\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('use_cuda: {}'.format(use_cuda))\n",
    "\n",
    "syncnet_T = 5\n",
    "syncnet_mel_step_size = 16\n",
    "\n",
    "# Rest of the code from wav2lip_train.py with modifications for GAN training\n",
    "\n",
    "# Add discriminator loss\n",
    "def train(device, model, disc_model, train_data_loader, test_data_loader, optimizer, disc_optimizer,\n",
    "          checkpoint_dir=None, checkpoint_interval=None, nepochs=None):\n",
    "    global global_step, global_epoch\n",
    "    resumed_step = global_step\n",
    " \n",
    "    while global_epoch < nepochs:\n",
    "        print('Starting Epoch: {}'.format(global_epoch))\n",
    "        running_sync_loss, running_l1_loss, running_disc_loss, running_perceptual_loss = 0., 0., 0., 0.\n",
    "        prog_bar = tqdm(enumerate(train_data_loader))\n",
    "        for step, (x, indiv_mels, mel, gt) in prog_bar:\n",
    "            model.train()\n",
    "            disc_model.train()\n",
    "            optimizer.zero_grad()\n",
    "            disc_optimizer.zero_grad()\n",
    "\n",
    "            # Move data to CUDA device\n",
    "            x = x.to(device)\n",
    "            mel = mel.to(device)\n",
    "            indiv_mels = indiv_mels.to(device)\n",
    "            gt = gt.to(device)\n",
    "\n",
    "            g = model(indiv_mels, x)\n",
    "\n",
    "            # Sync loss\n",
    "            if hparams.syncnet_wt > 0.:\n",
    "                sync_loss = get_sync_loss(mel, g)\n",
    "            else:\n",
    "                sync_loss = 0.\n",
    "\n",
    "            # L1 loss\n",
    "            l1loss = recon_loss(g, gt)\n",
    "\n",
    "            # Perceptual loss (GAN)\n",
    "            if hparams.disc_wt > 0.:\n",
    "                perceptual_loss = disc_model.perceptual_forward(g)\n",
    "            else:\n",
    "                perceptual_loss = 0.\n",
    "\n",
    "            # Combined loss\n",
    "            loss = hparams.syncnet_wt * sync_loss + (1 - hparams.syncnet_wt - hparams.disc_wt) * l1loss\n",
    "            if hparams.disc_wt > 0.:\n",
    "                loss += hparams.disc_wt * perceptual_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Train discriminator\n",
    "            if hparams.disc_wt > 0.:\n",
    "                disc_optimizer.zero_grad()\n",
    "                \n",
    "                # Real samples - should be classified as real (1)\n",
    "                real_pred = disc_model(gt)\n",
    "                real_loss = F.binary_cross_entropy(real_pred, torch.ones((len(real_pred), 1)).to(device))\n",
    "                \n",
    "                # Fake samples - should be classified as fake (0)\n",
    "                fake_pred = disc_model(g.detach())\n",
    "                fake_loss = F.binary_cross_entropy(fake_pred, torch.zeros((len(fake_pred), 1)).to(device))\n",
    "                \n",
    "                disc_loss = (real_loss + fake_loss) / 2\n",
    "                disc_loss.backward()\n",
    "                disc_optimizer.step()\n",
    "            else:\n",
    "                disc_loss = 0.\n",
    "\n",
    "            # Rest of the training loop...\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    checkpoint_dir = args.checkpoint_dir\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Dataset and Dataloader setup\n",
    "    train_dataset = Dataset('train')\n",
    "    test_dataset = Dataset('val')\n",
    "\n",
    "    train_data_loader = data_utils.DataLoader(\n",
    "        train_dataset, batch_size=hparams.batch_size, shuffle=True,\n",
    "        num_workers=hparams.num_workers)\n",
    "\n",
    "    test_data_loader = data_utils.DataLoader(\n",
    "        test_dataset, batch_size=hparams.batch_size,\n",
    "        num_workers=4)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    # Model\n",
    "    model = Wav2Lip().to(device)\n",
    "    disc_model = Discriminator().to(device)\n",
    "    \n",
    "    print('Generator: total trainable params {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "    print('Discriminator: total trainable params {}'.format(sum(p.numel() for p in disc_model.parameters() if p.requires_grad)))\n",
    "\n",
    "    optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                           lr=hparams.initial_learning_rate)\n",
    "    disc_optimizer = optim.Adam([p for p in disc_model.parameters() if p.requires_grad],\n",
    "                           lr=hparams.disc_initial_learning_rate)\n",
    "\n",
    "    if args.checkpoint_path is not None:\n",
    "        load_checkpoint(args.checkpoint_path, model, optimizer, reset_optimizer=False)\n",
    "        \n",
    "    if args.disc_checkpoint_path is not None:\n",
    "        load_checkpoint(args.disc_checkpoint_path, disc_model, disc_optimizer, reset_optimizer=False, overwrite_global_states=False)\n",
    "        \n",
    "    load_checkpoint(args.syncnet_checkpoint_path, syncnet, None, reset_optimizer=True, overwrite_global_states=False)\n",
    "\n",
    "    # Train!\n",
    "    train(device, model, disc_model, train_data_loader, test_data_loader, optimizer, disc_optimizer,\n",
    "              checkpoint_dir=checkpoint_dir,\n",
    "              checkpoint_interval=hparams.checkpoint_interval,\n",
    "              nepochs=hparams.nepochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a05a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from models import Wav2Lip\n",
    "import face_detection\n",
    "import audio\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Inference with improved post-processing')\n",
    "parser.add_argument('--checkpoint_path', help='Path to the Wav2Lip model checkpoint', required=True)\n",
    "parser.add_argument('--face', help='Path to video/image that contains faces to use', required=True)\n",
    "parser.add_argument('--audio', help='Path to audio file to use', required=True)\n",
    "parser.add_argument('--outfile', help='Path to save the output file', required=True)\n",
    "parser.add_argument('--smooth_factor', help='Smoothing factor for blending', default=0.8, type=float)\n",
    "parser.add_argument('--enhance_face', help='Apply face enhancement', action='store_true')\n",
    "args = parser.parse_args()\n",
    "\n",
    "def get_smoothened_boxes(boxes, T):\n",
    "    \"\"\"Smooth detection boxes across frames\"\"\"\n",
    "    for i in range(len(boxes)):\n",
    "        if i > 0 and i < len(boxes) - 1:\n",
    "            boxes[i] = 0.5 * boxes[i] + 0.25 * boxes[i-1] + 0.25 * boxes[i+1]\n",
    "    return boxes\n",
    "\n",
    "def face_detect(images):\n",
    "    \"\"\"Detect faces in images\"\"\"\n",
    "    detector = face_detection.FaceAlignment(face_detection.LandmarksType._2D, \n",
    "                                          flip_input=False, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    batch_size = 16\n",
    "    while batch_size > 0:\n",
    "        try:\n",
    "            predictions = []\n",
    "            for i in tqdm(range(0, len(images), batch_size)):\n",
    "                batch = images[i:i + batch_size]\n",
    "                results = detector.get_detections_for_batch(np.array(batch))\n",
    "                predictions.extend(results)\n",
    "            break\n",
    "        except RuntimeError:\n",
    "            batch_size //= 2\n",
    "    \n",
    "    results = []\n",
    "    pady1, pady2, padx1, padx2 = [0, 10, 0, 0]\n",
    "    for rect, image in zip(predictions, images):\n",
    "        if rect is None:\n",
    "            continue\n",
    "        \n",
    "        y1 = max(0, rect[1] - pady1)\n",
    "        y2 = min(image.shape[0], rect[3] + pady2)\n",
    "        x1 = max(0, rect[0] - padx1)\n",
    "        x2 = min(image.shape[1], rect[2] + padx2)\n",
    "        \n",
    "        results.append([x1, y1, x2, y2])\n",
    "    \n",
    "    boxes = get_smoothened_boxes(results, T=5)\n",
    "    return boxes\n",
    "\n",
    "def enhance_face(face):\n",
    "    \"\"\"Apply simple face enhancement\"\"\"\n",
    "    # Apply subtle sharpening\n",
    "    kernel = np.array([[-1, -1, -1], \n",
    "                       [-1,  9, -1],\n",
    "                       [-1, -1, -1]])\n",
    "    sharpened = cv2.filter2D(face, -1, kernel)\n",
    "    \n",
    "    # Blend with original\n",
    "    enhanced = cv2.addWeighted(face, 0.7, sharpened, 0.3, 0)\n",
    "    \n",
    "    # Subtle color correction\n",
    "    lab = cv2.cvtColor(enhanced, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    cl = clahe.apply(l)\n",
    "    merged = cv2.merge([cl, a, b])\n",
    "    enhanced = cv2.cvtColor(merged, cv2.COLOR_LAB2BGR)\n",
    "    \n",
    "    return enhanced\n",
    "\n",
    "def main():\n",
    "    # Load model\n",
    "    model = Wav2Lip()\n",
    "    checkpoint = torch.load(args.checkpoint_path)\n",
    "    s = checkpoint[\"state_dict\"]\n",
    "    new_s = {}\n",
    "    for k, v in s.items():\n",
    "        new_s[k.replace('module.', '')] = v\n",
    "    model.load_state_dict(new_s)\n",
    "    model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "    \n",
    "    # Load video and audio\n",
    "    video_stream = cv2.VideoCapture(args.face)\n",
    "    fps = video_stream.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Read video frames\n",
    "    full_frames = []\n",
    "    while True:\n",
    "        ret, frame = video_stream.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        full_frames.append(frame)\n",
    "    \n",
    "    # Detect faces\n",
    "    face_boxes = face_detect(full_frames)\n",
    "    \n",
    "    # Process audio\n",
    "    wav = audio.load_wav(args.audio, 16000)\n",
    "    mel = audio.melspectrogram(wav)\n",
    "    \n",
    "    # Process each frame\n",
    "    output_frames = []\n",
    "    for i, (frame, bbox) in enumerate(tqdm(zip(full_frames, face_boxes), total=len(full_frames))):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        face = frame[y1:y2, x1:x2]\n",
    "        \n",
    "        # Prepare face for model\n",
    "        face = cv2.resize(face, (96, 96))\n",
    "        face = np.transpose(face, (2, 0, 1))\n",
    "        face = face / 255.0\n",
    "        face = torch.FloatTensor(face).unsqueeze(0).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Get corresponding audio segment\n",
    "        frame_idx = i\n",
    "        start_idx = int(80. * (frame_idx / float(fps)))\n",
    "        end_idx = start_idx + 16\n",
    "        if end_idx > len(mel):\n",
    "            break\n",
    "        mel_segment = torch.FloatTensor(mel[start_idx:end_idx]).unsqueeze(0).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Generate lip-synced face\n",
    "        with torch.no_grad():\n",
    "            pred = model(mel_segment, face)\n",
    "        \n",
    "        # Convert back to numpy\n",
    "        pred = pred.cpu().numpy().transpose(0, 2, 3, 1)[0] * 255.\n",
    "        pred = pred.astype(np.uint8)\n",
    "        \n",
    "        # Apply face enhancement if requested\n",
    "        if args.enhance_face:\n",
    "            pred = enhance_face(pred)\n",
    "        \n",
    "        # Resize back to original size\n",
    "        pred = cv2.resize(pred, (x2 - x1, y2 - y1))\n",
    "        \n",
    "        # Create a mask for smooth blending\n",
    "        mask = np.zeros((y2 - y1, x2 - x1), dtype=np.float32)\n",
    "        mask[int(mask.shape[0]*0.3):] = 1.0  # Only blend the lower part of the face (mouth region)\n",
    "        mask = cv2.GaussianBlur(mask, (15, 15), 5)\n",
    "        mask = np.expand_dims(mask, -1)\n",
    "        \n",
    "        # Blend the generated face with the original using the mask\n",
    "        blended_face = args.smooth_factor * pred + (1 - args.smooth_factor) * frame[y1:y2, x1:x2]\n",
    "        blended_face = blended_face.astype(np.uint8)\n",
    "        \n",
    "        # Copy the blended face back to the original frame\n",
    "        output_frame = frame.copy()\n",
    "        output_frame[y1:y2, x1:x2] = blended_face\n",
    "        \n",
    "        output_frames.append(output_frame)\n",
    "    \n",
    "    # Write output video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(args.outfile, fourcc, fps, (full_frames[0].shape[1], full_frames[0].shape[0]))\n",
    "    for frame in output_frames:\n",
    "        out.write(frame)\n",
    "    out.release()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3240ee1c",
   "metadata": {},
   "source": [
    "@echo off\n",
    "echo Starting Wav2Lip+GAN training pipeline\n",
    "\n",
    "set DATA_ROOT=E:\\training data\n",
    "set OUTPUT_DIR=E:\\play_6.0\\processed_data\n",
    "set CHECKPOINT_DIR=E:\\play_6.0\\checkpoints\n",
    "set SYNCNET_PATH=E:\\play_6.0\\Wav2Lip\\checkpoints\\syncnet.pth\n",
    "\n",
    "echo Step 1: Preprocessing data...\n",
    "python preprocess.py --data_root \"%DATA_ROOT%\" --output_dir \"%OUTPUT_DIR%\"\n",
    "\n",
    "echo Step 2: Training Wav2Lip model...\n",
    "python train_wav2lip.py --data_root \"%OUTPUT_DIR%\" --checkpoint_dir \"%CHECKPOINT_DIR%\" --syncnet_checkpoint_path \"%SYNCNET_PATH%\"\n",
    "\n",
    "echo Step 3: Training Wav2Lip+GAN model...\n",
    "python train_wav2lip_gan.py --data_root \"%OUTPUT_DIR%\" --checkpoint_dir \"%CHECKPOINT_DIR%\\gan\" --syncnet_checkpoint_path \"%SYNCNET_PATH%\" --checkpoint_path \"%CHECKPOINT_DIR%\\checkpoint_latest.pth\"\n",
    "\n",
    "echo Training complete! Models saved to %CHECKPOINT_DIR%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wav2lip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
