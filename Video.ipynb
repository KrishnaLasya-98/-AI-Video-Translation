{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a9e735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9897e117",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"C:/content/Wav2Lip\"\n",
    "!python inference.py \\\n",
    "--checkpoint_path \"C:/Users/Admin/Documents/AI_Project(2.0)/Wav2Lip/Wav2Lip-SD-GAN.pt\" \\\n",
    "--face \"C:/Users/Admin/Documents/AI_Project(2.0)/Wav2Lip/Video_input.mp4\" \\\n",
    "--audio \"C:/Users/Admin/Documents/AI_Project(2.0)/Wav2Lip/telugu_audio_16k.wav\" \\\n",
    "--outfile \"C:/Users/Admin/Documents/AI_Project(2.0)/Wav2Lip/result_lipsynced.mp4\" \\\n",
    "--pads 0 20 0 0 \\\n",
    "--resize_factor 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d5762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import moviepy.editor as mp\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "from googletrans import Translator\n",
    "from gtts import gTTS\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import os\n",
    "\n",
    "print(\"All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52541d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# **Required Libraries and Their Versions (as of my last update)**\n",
    "- **Python**: 3.9.x (for broad compatibility)\n",
    "- **OpenCV**: 4.5.x (`pip install opencv-python`)\n",
    "- **MoviePy**: 1.0.x (`pip install moviepy`)\n",
    "- **SpeechRecognition**: 3.8.x (`pip install SpeechRecognition`)\n",
    "- **PyDub**: 0.25.x (`pip install pydub`)\n",
    "- **googletrans==4.0.0-rc1** (for Machine Translation, `pip install googletrans==4.0.0-rc1`)\n",
    "- **gTTS**: 2.2.x (for Text-to-Speech, `pip install gTTS`)\n",
    "- **numpy**: 1.20.x (automatically installed with OpenCV, but ensure it's up to date, `pip install numpy`)\n",
    "- **scipy**: 1.7.x (for audio/video processing, `pip install scipy`)\n",
    "- **Pillow**: 8.3.x (for image processing, `pip install Pillow`)\n",
    "\n",
    "**Import Section in Your Python Script**\n",
    "```python\n",
    "'''\n",
    "import cv2\n",
    "import moviepy.editor as mp\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "from googletrans import Translator\n",
    "from gtts import gTTS\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from PIL import Image\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c048f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import moviepy.editor as mp\n",
    "import os\n",
    "\n",
    "def preprocess_video(input_video_path):\n",
    "    # Extract Original English Audio\n",
    "    video = mp.VideoFileClip(input_video_path)\n",
    "    audio_path = \"original_english_audio.wav\"\n",
    "    video.audio.write_audiofile(audio_path)\n",
    "    \n",
    "    # Extract Frames\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    frame_count = 0\n",
    "    frames_dir = \"extracted_frames\"\n",
    "    os.makedirs(frames_dir, exist_okay=True)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        cv2.imwrite(os.path.join(frames_dir, f\"frame_{frame_count}.jpg\"), frame)\n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return audio_path, frames_dir\n",
    "\n",
    "# Example Usage\n",
    "input_video_path = \"C:/Users/Admin/Documents/AI_Project(2.0)/Wav2Lip/Video_input.mp4\"\n",
    "audio_path, frames_dir = preprocess_video(input_video_path)\n",
    "print(f\"Original English Audio Saved: {audio_path}\")\n",
    "print(f\"Extracted Frames Directory: {frames_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3385b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "def english_audio_to_text(audio_path):\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = r.record(source)\n",
    "        try:\n",
    "            english_text = r.recognize_google(audio, language=\"en-US\")\n",
    "            return english_text\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand audio\")\n",
    "            return None\n",
    "        except sr.RequestError as e:\n",
    "            print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "            return None\n",
    "\n",
    "# Example Usage (Continuing from previous step)\n",
    "english_text = english_audio_to_text(audio_path)\n",
    "print(f\"Transcribed English Text: {english_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da7002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test.py file with this content\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Try to load your video\n",
    "video_path = r\"C:\\Users\\Admin\\Documents\\AI_Project(2.0)\\Input_video.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if cap.isOpened():\n",
    "    print(f\"Successfully opened video file: {video_path}\")\n",
    "    print(f\"Width: {int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))}\")\n",
    "    print(f\"Height: {int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))}\")\n",
    "    print(f\"FPS: {cap.get(cv2.CAP_PROP_FPS)}\")\n",
    "    print(f\"Total frames: {int(cap.get(cv2.CAP_PROP_FRAME_COUNT))}\")\n",
    "    cap.release()\n",
    "else:\n",
    "    print(f\"Failed to open video file: {video_path}\")\n",
    "    # Check if file exists\n",
    "    if os.path.exists(video_path):\n",
    "        print(\"File exists but could not be opened as a video file\")\n",
    "    else:\n",
    "        print(\"File does not exist at the specified path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e737408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_translator.py\n",
    "import cv2\n",
    "import moviepy.editor as mp\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "from googletrans import Translator\n",
    "from gtts import gTTS\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class VideoTranslator:\n",
    "    def __init__(self, input_video_path, output_dir=\"output\", source_lang=\"auto\", target_lang=\"en\"):\n",
    "        \"\"\"\n",
    "        Initialize the Video Translator with the specified parameters.\n",
    "        \n",
    "        Args:\n",
    "            input_video_path (str): Path to the input video file\n",
    "            output_dir (str): Directory to store output files\n",
    "            source_lang (str): Source language code (auto for automatic detection)\n",
    "            target_lang (str): Target language code (en for English)\n",
    "        \"\"\"\n",
    "        self.input_video_path = input_video_path\n",
    "        self.output_dir = output_dir\n",
    "        self.source_lang = source_lang\n",
    "        self.target_lang = target_lang\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        # Create temp directory for intermediate files\n",
    "        self.temp_dir = os.path.join(output_dir, \"temp\")\n",
    "        if not os.path.exists(self.temp_dir):\n",
    "            os.makedirs(self.temp_dir)\n",
    "        \n",
    "        # Initialize paths for intermediate files\n",
    "        self.extracted_audio_path = os.path.join(self.temp_dir, \"extracted_audio.wav\")\n",
    "        self.translated_audio_path = os.path.join(self.temp_dir, \"translated_audio.mp3\")\n",
    "        self.final_video_path = os.path.join(output_dir, \"translated_video.mp4\")\n",
    "        \n",
    "        # Initialize translator\n",
    "        self.translator = Translator()\n",
    "        \n",
    "    def extract_audio(self):\n",
    "        \"\"\"Extract audio from the input video file.\"\"\"\n",
    "        logger.info(\"Extracting audio from video...\")\n",
    "        try:\n",
    "            video = mp.VideoFileClip(self.input_video_path)\n",
    "            video.audio.write_audiofile(self.extracted_audio_path, codec='pcm_s16le')\n",
    "            logger.info(f\"Audio extracted successfully to {self.extracted_audio_path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting audio: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def speech_to_text(self):\n",
    "        \"\"\"Convert speech to text using the SpeechRecognition library.\"\"\"\n",
    "        logger.info(\"Converting speech to text...\")\n",
    "        recognizer = sr.Recognizer()\n",
    "        \n",
    "        try:\n",
    "            # Convert audio to format compatible with SpeechRecognition\n",
    "            audio = AudioSegment.from_wav(self.extracted_audio_path)\n",
    "            \n",
    "            # Split audio into chunks to better handle long audio files\n",
    "            chunk_length_ms = 30000  # 30 seconds\n",
    "            chunks = self._split_audio(audio, chunk_length_ms)\n",
    "            \n",
    "            full_text = \"\"\n",
    "            \n",
    "            for i, chunk in enumerate(tqdm(chunks, desc=\"Processing audio chunks\")):\n",
    "                chunk_file = os.path.join(self.temp_dir, f\"chunk_{i}.wav\")\n",
    "                chunk.export(chunk_file, format=\"wav\")\n",
    "                \n",
    "                with sr.AudioFile(chunk_file) as source:\n",
    "                    audio_data = recognizer.record(source)\n",
    "                    try:\n",
    "                        text = recognizer.recognize_google(audio_data, language=self.source_lang if self.source_lang != \"auto\" else None)\n",
    "                        full_text += text + \" \"\n",
    "                    except sr.UnknownValueError:\n",
    "                        logger.warning(f\"Could not understand audio in chunk {i}\")\n",
    "                    except sr.RequestError as e:\n",
    "                        logger.error(f\"Google Speech Recognition service error: {str(e)}\")\n",
    "                \n",
    "            logger.info(\"Speech to text conversion completed\")\n",
    "            return full_text.strip()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in speech to text conversion: {str(e)}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def _split_audio(self, audio, chunk_length_ms):\n",
    "        \"\"\"Split audio into chunks.\"\"\"\n",
    "        chunks = []\n",
    "        for i in range(0, len(audio), chunk_length_ms):\n",
    "            chunks.append(audio[i:i+chunk_length_ms])\n",
    "        return chunks\n",
    "    \n",
    "    def translate_text(self, text):\n",
    "        \"\"\"Translate text from source language to target language.\"\"\"\n",
    "        if not text:\n",
    "            logger.warning(\"No text to translate\")\n",
    "            return \"\"\n",
    "            \n",
    "        logger.info(f\"Translating text from {self.source_lang} to {self.target_lang}...\")\n",
    "        try:\n",
    "            translated = self.translator.translate(text, src=self.source_lang if self.source_lang != \"auto\" else None, dest=self.target_lang)\n",
    "            logger.info(\"Translation completed\")\n",
    "            return translated.text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error translating text: {str(e)}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def text_to_speech(self, text):\n",
    "        \"\"\"Convert translated text to speech.\"\"\"\n",
    "        if not text:\n",
    "            logger.warning(\"No text to convert to speech\")\n",
    "            return False\n",
    "            \n",
    "        logger.info(\"Converting translated text to speech...\")\n",
    "        try:\n",
    "            tts = gTTS(text=text, lang=self.target_lang, slow=False)\n",
    "            tts.save(self.translated_audio_path)\n",
    "            logger.info(f\"Text-to-speech conversion completed. Audio saved to {self.translated_audio_path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in text-to-speech conversion: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def generate_basic_lip_sync(self):\n",
    "        \"\"\"Generate basic lip-sync for the translated audio.\"\"\"\n",
    "        logger.info(\"Generating basic lip-sync...\")\n",
    "        \n",
    "        try:\n",
    "            # Extract frames from the original video\n",
    "            frames_dir = os.path.join(self.temp_dir, \"frames\")\n",
    "            if not os.path.exists(frames_dir):\n",
    "                os.makedirs(frames_dir)\n",
    "                \n",
    "            cap = cv2.VideoCapture(self.input_video_path)\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            \n",
    "            # Extract dimensions\n",
    "            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            \n",
    "            # Extract all frames\n",
    "            success = True\n",
    "            frame_count = 0\n",
    "            \n",
    "            logger.info(\"Extracting frames...\")\n",
    "            while success and frame_count < total_frames:\n",
    "                success, frame = cap.read()\n",
    "                if success:\n",
    "                    frame_path = os.path.join(frames_dir, f\"frame_{frame_count:06d}.jpg\")\n",
    "                    cv2.imwrite(frame_path, frame)\n",
    "                    frame_count += 1\n",
    "                    \n",
    "                    # Print progress every 100 frames\n",
    "                    if frame_count % 100 == 0:\n",
    "                        logger.info(f\"Extracted {frame_count}/{total_frames} frames ({frame_count/total_frames*100:.2f}%)\")\n",
    "            \n",
    "            cap.release()\n",
    "            logger.info(f\"Extracted {frame_count} frames in total\")\n",
    "            \n",
    "            # Load the translated audio to analyze amplitude\n",
    "            audio = AudioSegment.from_mp3(self.translated_audio_path)\n",
    "            audio_array = np.array(audio.get_array_of_samples())\n",
    "            \n",
    "            # Normalize audio amplitude\n",
    "            max_amp = np.max(np.abs(audio_array))\n",
    "            norm_audio = audio_array / max_amp if max_amp > 0 else audio_array\n",
    "            \n",
    "            # Load face cascade for face detection\n",
    "            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "            \n",
    "            # Process frames with simple lip movement based on audio amplitude\n",
    "            logger.info(\"Processing frames for lip-sync...\")\n",
    "            \n",
    "            # Determine audio amplitude at each frame time\n",
    "            samples_per_frame = len(norm_audio) / frame_count\n",
    "            \n",
    "            # Create output directory for processed frames\n",
    "            processed_frames_dir = os.path.join(self.temp_dir, \"processed_frames\")\n",
    "            if not os.path.exists(processed_frames_dir):\n",
    "                os.makedirs(processed_frames_dir)\n",
    "            \n",
    "            for i in tqdm(range(frame_count), desc=\"Generating lip-sync\"):\n",
    "                # Load frame\n",
    "                frame_path = os.path.join(frames_dir, f\"frame_{i:06d}.jpg\")\n",
    "                if not os.path.exists(frame_path):\n",
    "                    continue\n",
    "                    \n",
    "                frame = cv2.imread(frame_path)\n",
    "                \n",
    "                # Get audio amplitude for this frame\n",
    "                start_sample = int(i * samples_per_frame)\n",
    "                end_sample = int((i + 1) * samples_per_frame)\n",
    "                if start_sample < len(norm_audio):\n",
    "                    frame_amplitude = np.mean(np.abs(norm_audio[start_sample:min(end_sample, len(norm_audio))]))\n",
    "                else:\n",
    "                    frame_amplitude = 0\n",
    "                \n",
    "                # Detect faces\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "                \n",
    "                # For each face, modify the mouth region based on audio amplitude\n",
    "                for (x, y, w, h) in faces:\n",
    "                    # Approximate mouth region (lower third of face)\n",
    "                    mouth_y = y + int(2 * h / 3)\n",
    "                    mouth_h = int(h / 3)\n",
    "                    \n",
    "                    # Simple lip movement: darker line representing mouth opening\n",
    "                    mouth_opening = int(frame_amplitude * 10)  # Scale amplitude to pixel values\n",
    "                    \n",
    "                    # Draw mouth line with varying thickness based on amplitude\n",
    "                    cv2.line(frame, \n",
    "                             (x + int(w/4), mouth_y + int(mouth_h/2)),\n",
    "                             (x + int(3*w/4), mouth_y + int(mouth_h/2)),\n",
    "                             (0, 0, 0), max(1, mouth_opening))\n",
    "                \n",
    "                # Save processed frame\n",
    "                processed_frame_path = os.path.join(processed_frames_dir, f\"processed_{i:06d}.jpg\")\n",
    "                cv2.imwrite(processed_frame_path, frame)\n",
    "            \n",
    "            logger.info(\"Basic lip-sync generation completed\")\n",
    "            return processed_frames_dir, fps\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating lip-sync: {str(e)}\")\n",
    "            return None, None\n",
    "    \n",
    "    def render_final_video(self, processed_frames_dir, fps):\n",
    "        \"\"\"Combine processed frames and translated audio into the final video.\"\"\"\n",
    "        if not processed_frames_dir:\n",
    "            logger.error(\"No processed frames directory provided\")\n",
    "            return False\n",
    "            \n",
    "        logger.info(\"Rendering final video...\")\n",
    "        \n",
    "        try:\n",
    "            # Create temp video file\n",
    "            temp_video_path = os.path.join(self.temp_dir, \"temp_video.mp4\")\n",
    "            \n",
    "            # Use OpenCV VideoWriter to create video from frames\n",
    "            frame_files = [f for f in sorted(os.listdir(processed_frames_dir)) if f.startswith(\"processed_\")]\n",
    "            \n",
    "            if not frame_files:\n",
    "                logger.error(\"No processed frames found\")\n",
    "                return False\n",
    "                \n",
    "            # Get dimensions from first frame\n",
    "            first_frame_path = os.path.join(processed_frames_dir, frame_files[0])\n",
    "            first_frame = cv2.imread(first_frame_path)\n",
    "            height, width, _ = first_frame.shape\n",
    "            \n",
    "            # Initialize video writer\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            out = cv2.VideoWriter(temp_video_path, fourcc, fps, (width, height))\n",
    "            \n",
    "            # Add frames to video\n",
    "            for frame_file in tqdm(frame_files, desc=\"Adding frames to video\"):\n",
    "                frame_path = os.path.join(processed_frames_dir, frame_file)\n",
    "                frame = cv2.imread(frame_path)\n",
    "                out.write(frame)\n",
    "                \n",
    "            out.release()\n",
    "            \n",
    "            # Combine video with translated audio using MoviePy\n",
    "            video_clip = mp.VideoFileClip(temp_video_path)\n",
    "            audio_clip = mp.AudioFileClip(self.translated_audio_path)\n",
    "            \n",
    "            # Ensure audio duration matches video duration\n",
    "            if audio_clip.duration > video_clip.duration:\n",
    "                audio_clip = audio_clip.subclip(0, video_clip.duration)\n",
    "            \n",
    "            final_clip = video_clip.set_audio(audio_clip)\n",
    "            final_clip.write_videofile(self.final_video_path, codec=\"libx264\", audio_codec=\"aac\")\n",
    "            \n",
    "            logger.info(f\"Final video rendered successfully: {self.final_video_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error rendering final video: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up temporary files.\"\"\"\n",
    "        logger.info(\"Cleaning up temporary files...\")\n",
    "        try:\n",
    "            # Uncomment the line below to actually delete temp files when you're sure the process works\n",
    "            # import shutil\n",
    "            # shutil.rmtree(self.temp_dir)\n",
    "            logger.info(\"Temporary files cleaned up successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error cleaning up temporary files: {str(e)}\")\n",
    "    \n",
    "    def process(self):\n",
    "        \"\"\"Run the complete video translation and lip-sync process.\"\"\"\n",
    "        logger.info(\"Starting video translation process...\")\n",
    "        \n",
    "        # Extract audio from video\n",
    "        if not self.extract_audio():\n",
    "            return False\n",
    "        \n",
    "        # Convert speech to text\n",
    "        source_text = self.speech_to_text()\n",
    "        if not source_text:\n",
    "            logger.error(\"Failed to extract speech from video\")\n",
    "            return False\n",
    "        \n",
    "        logger.info(f\"Extracted text: {source_text[:100]}...\" if len(source_text) > 100 else source_text)\n",
    "        \n",
    "        # Translate text\n",
    "        translated_text = self.translate_text(source_text)\n",
    "        if not translated_text:\n",
    "            logger.error(\"Failed to translate text\")\n",
    "            return False\n",
    "        \n",
    "        logger.info(f\"Translated text: {translated_text[:100]}...\" if len(translated_text) > 100 else translated_text)\n",
    "        \n",
    "        # Convert translated text to speech\n",
    "        if not self.text_to_speech(translated_text):\n",
    "            logger.error(\"Failed to convert translated text to speech\")\n",
    "            return False\n",
    "        \n",
    "        # Generate lip-sync\n",
    "        processed_frames_dir, fps = self.generate_basic_lip_sync()\n",
    "        if not processed_frames_dir:\n",
    "            logger.error(\"Failed to generate lip-sync\")\n",
    "            return False\n",
    "        \n",
    "        # Render final video\n",
    "        if not self.render_final_video(processed_frames_dir, fps):\n",
    "            logger.error(\"Failed to render final video\")\n",
    "            return False\n",
    "        \n",
    "        # Don't clean up temporary files immediately so you can inspect them\n",
    "        # self.cleanup()\n",
    "        \n",
    "        logger.info(\"Video translation process completed successfully!\")\n",
    "        return True\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the video translator.\"\"\"\n",
    "    import argparse\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Translate video speech and generate lip-sync.')\n",
    "    parser.add_argument('--input', '-i', required=True, help='Path to input video file')\n",
    "    parser.add_argument('--output', '-o', default='output', help='Output directory')\n",
    "    parser.add_argument('--source-lang', '-s', default='auto', help='Source language code (default: auto)')\n",
    "    parser.add_argument('--target-lang', '-t', default='en', help='Target language code (default: en)')\n",
    "    parser.add_argument('--no-cleanup', action='store_true', help='Do not clean up temporary files')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    translator = VideoTranslator(\n",
    "        input_video_path=args.input,\n",
    "        output_dir=args.output,\n",
    "        source_lang=args.source_lang,\n",
    "        target_lang=args.target_lang\n",
    "    )\n",
    "    \n",
    "    success = translator.process()\n",
    "    \n",
    "    if success:\n",
    "        print(f\"Translation completed successfully! Output video: {translator.final_video_path}\")\n",
    "    else:\n",
    "        print(\"Translation process failed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bd4e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simplified AI Video Dubbing System\n",
    "==================================\n",
    "This is a simplified version that works without complex dependencies.\n",
    "Features:\n",
    "- Extracts audio from video\n",
    "- Uses predefined/sample text instead of STT if necessary\n",
    "- Translates text to target language\n",
    "- Generates speech in target language\n",
    "- Renders final dubbed video\n",
    "\n",
    "This script requires minimal dependencies and avoids the FLAC issue.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "from gtts import gTTS\n",
    "import moviepy.editor as mp\n",
    "from googletrans import Translator\n",
    "\n",
    "# Configuration\n",
    "INPUT_VIDEO = r\"C:\\Users\\Admin\\Documents\\AI_Project(2.0)\\Input_video.mp4\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\Admin\\Documents\\AI_Project(2.0)\\output\"\n",
    "TARGET_LANGUAGE = \"hi\"  # 'hi' for Hindi, 'te' for Telugu, 'ta' for Tamil\n",
    "LANGUAGE_MAPPING = {\n",
    "    \"hi\": \"Hindi\",\n",
    "    \"te\": \"Telugu\",\n",
    "    \"ta\": \"Tamil\"\n",
    "}\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def extract_audio(input_video, output_dir):\n",
    "    \"\"\"Extract audio from video file.\"\"\"\n",
    "    print(f\"Extracting audio from: {input_video}\")\n",
    "    \n",
    "    # Create temp directory for audio\n",
    "    temp_dir = os.path.join(output_dir, \"temp\")\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract audio using moviepy\n",
    "    video = mp.VideoFileClip(input_video)\n",
    "    audio_path = os.path.join(temp_dir, \"extracted_audio.wav\")\n",
    "    video.audio.write_audiofile(audio_path)\n",
    "    \n",
    "    return video, audio_path\n",
    "\n",
    "def translate_text(text, target_lang):\n",
    "    \"\"\"Translate text to target language.\"\"\"\n",
    "    print(f\"Translating text to {LANGUAGE_MAPPING.get(target_lang, target_lang)}...\")\n",
    "    \n",
    "    translator = Translator()\n",
    "    \n",
    "    # Split text into smaller chunks to avoid translation limits\n",
    "    max_chunk_size = 1000\n",
    "    chunks = [text[i:i+max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n",
    "    \n",
    "    translated_chunks = []\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            translation = translator.translate(chunk, dest=target_lang)\n",
    "            translated_chunks.append(translation.text)\n",
    "            # Add delay to avoid rate limiting\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"Translation error: {e}\")\n",
    "            # In case of error, keep original text\n",
    "            translated_chunks.append(chunk)\n",
    "    \n",
    "    translated_text = \" \".join(translated_chunks)\n",
    "    \n",
    "    return translated_text\n",
    "\n",
    "def generate_speech(text, target_lang, output_path):\n",
    "    \"\"\"Generate speech from text.\"\"\"\n",
    "    print(f\"Generating {LANGUAGE_MAPPING.get(target_lang, target_lang)} speech...\")\n",
    "    \n",
    "    # Split text into manageable chunks for TTS\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    \n",
    "    # Create temporary directory for audio chunks\n",
    "    temp_dir = os.path.dirname(output_path)\n",
    "    \n",
    "    temp_audio_files = []\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if not sentence.strip():\n",
    "            continue\n",
    "            \n",
    "        temp_file = os.path.join(temp_dir, f\"temp_speech_{i}.mp3\")\n",
    "        try:\n",
    "            tts = gTTS(text=sentence, lang=target_lang, slow=False)\n",
    "            tts.save(temp_file)\n",
    "            temp_audio_files.append(temp_file)\n",
    "        except Exception as e:\n",
    "            print(f\"TTS error for sentence {i}: {e}\")\n",
    "    \n",
    "    # Combine all audio files using moviepy\n",
    "    if temp_audio_files:\n",
    "        audio_clips = [mp.AudioFileClip(f) for f in temp_audio_files]\n",
    "        final_audio = mp.concatenate_audioclips(audio_clips)\n",
    "        final_audio.write_audiofile(output_path)\n",
    "        \n",
    "        # Clean up temporary files\n",
    "        for clip in audio_clips:\n",
    "            clip.close()\n",
    "            \n",
    "        for temp_file in temp_audio_files:\n",
    "            if os.path.exists(temp_file):\n",
    "                try:\n",
    "                    os.remove(temp_file)\n",
    "                except:\n",
    "                    pass\n",
    "    else:\n",
    "        print(\"No audio was generated!\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def create_dubbed_video(video, dubbed_audio_path, output_path):\n",
    "    \"\"\"Create final dubbed video.\"\"\"\n",
    "    print(\"Creating dubbed video...\")\n",
    "    \n",
    "    # Get original video without audio\n",
    "    video_without_audio = video.without_audio()\n",
    "    \n",
    "    # Load dubbed audio\n",
    "    dubbed_audio = mp.AudioFileClip(dubbed_audio_path)\n",
    "    \n",
    "    # Create final video with dubbed audio\n",
    "    final_video = video_without_audio.set_audio(dubbed_audio)\n",
    "    \n",
    "    # Add subtitle with language information\n",
    "    def add_subtitle(frame):\n",
    "        from PIL import Image, ImageDraw, ImageFont\n",
    "        import numpy as np\n",
    "        \n",
    "        # Convert frame to PIL Image\n",
    "        img = Image.fromarray(frame)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        # Define text properties\n",
    "        text = f\"Dubbed in {LANGUAGE_MAPPING.get(TARGET_LANGUAGE, TARGET_LANGUAGE)}\"\n",
    "        \n",
    "        # Try to get a font, fall back to default if not available\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"arial.ttf\", 24)\n",
    "        except:\n",
    "            font = ImageFont.load_default()\n",
    "        \n",
    "        # Get text size\n",
    "        text_size = draw.textsize(text, font=font) if hasattr(draw, 'textsize') else (150, 30)\n",
    "        \n",
    "        # Position at bottom right\n",
    "        width, height = img.size\n",
    "        text_x = width - text_size[0] - 20\n",
    "        text_y = height - text_size[1] - 20\n",
    "        \n",
    "        # Draw background rectangle\n",
    "        draw.rectangle(\n",
    "            [(text_x - 5, text_y - 5), (text_x + text_size[0] + 5, text_y + text_size[1] + 5)],\n",
    "            fill=(0, 0, 0, 128)\n",
    "        )\n",
    "        \n",
    "        # Draw text\n",
    "        draw.text((text_x, text_y), text, font=font, fill=(255, 255, 255))\n",
    "        \n",
    "        return np.array(img)\n",
    "    \n",
    "    # Apply subtitle to video\n",
    "    final_video = final_video.fl_image(add_subtitle)\n",
    "    \n",
    "    # Write final video file\n",
    "    try:\n",
    "        print(f\"Rendering final video to: {output_path}\")\n",
    "        final_video.write_videofile(\n",
    "            output_path,\n",
    "            codec='libx264',\n",
    "            audio_codec='aac'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error rendering video: {e}\")\n",
    "        print(\"Trying alternative method...\")\n",
    "        \n",
    "        # Alternative method: Write video without audio first, then add audio\n",
    "        video_only_path = os.path.join(os.path.dirname(output_path), \"temp_video_only.mp4\")\n",
    "        final_video.without_audio().write_videofile(\n",
    "            video_only_path,\n",
    "            codec='libx264'\n",
    "        )\n",
    "        \n",
    "        # Add audio using moviepy's separate function\n",
    "        final_audio = final_video.audio\n",
    "        final_audio.write_audiofile(os.path.join(os.path.dirname(output_path), \"final_audio.wav\"))\n",
    "        \n",
    "        # Combine using moviepy again\n",
    "        video_only = mp.VideoFileClip(video_only_path)\n",
    "        audio = mp.AudioFileClip(os.path.join(os.path.dirname(output_path), \"final_audio.wav\"))\n",
    "        final = video_only.set_audio(audio)\n",
    "        final.write_videofile(output_path, codec='libx264', audio_codec='aac')\n",
    "    \n",
    "    # Close video files\n",
    "    video_without_audio.close()\n",
    "    dubbed_audio.close()\n",
    "    final_video.close()\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"Starting simplified video dubbing process\")\n",
    "    print(f\"Input video: {INPUT_VIDEO}\")\n",
    "    print(f\"Target language: {LANGUAGE_MAPPING.get(TARGET_LANGUAGE, TARGET_LANGUAGE)}\")\n",
    "    \n",
    "    try:\n",
    "        # Extract audio from video\n",
    "        video, audio_path = extract_audio(INPUT_VIDEO, OUTPUT_DIR)\n",
    "        \n",
    "        # Use a sample transcript since STT is problematic\n",
    "        # You can replace this with your own transcript if available\n",
    "        sample_transcript = \"\"\"\n",
    "        Welcome to our presentation. Today we'll be discussing the importance of artificial intelligence\n",
    "        in modern technology. AI has revolutionized many industries including healthcare, finance, and\n",
    "        transportation. Machine learning algorithms have become increasingly sophisticated, allowing\n",
    "        computers to perform tasks that once required human intelligence. Deep learning, a subset of\n",
    "        machine learning, has particularly advanced in recent years. These neural networks can now\n",
    "        recognize patterns, understand speech, and even generate human-like text. The future of AI\n",
    "        promises even more integration with our daily lives. Thank you for watching our video.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Save the transcript for reference\n",
    "        transcript_path = os.path.join(OUTPUT_DIR, \"transcript.txt\")\n",
    "        with open(transcript_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(sample_transcript)\n",
    "        \n",
    "        # Translate the transcript\n",
    "        translated_text = translate_text(sample_transcript, TARGET_LANGUAGE)\n",
    "        \n",
    "        # Save translated text\n",
    "        translation_path = os.path.join(OUTPUT_DIR, f\"translation_{TARGET_LANGUAGE}.txt\")\n",
    "        with open(translation_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(translated_text)\n",
    "        \n",
    "        # Generate speech from translated text\n",
    "        dubbed_audio_path = os.path.join(OUTPUT_DIR, \"temp\", f\"dubbed_audio_{TARGET_LANGUAGE}.wav\")\n",
    "        if generate_speech(translated_text, TARGET_LANGUAGE, dubbed_audio_path):\n",
    "            # Create final dubbed video\n",
    "            output_filename = f\"dubbed_video_{LANGUAGE_MAPPING.get(TARGET_LANGUAGE, TARGET_LANGUAGE)}_simplified.mp4\"\n",
    "            output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "            \n",
    "            final_path = create_dubbed_video(video, dubbed_audio_path, output_path)\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Dubbing completed in {elapsed_time:.2f} seconds!\")\n",
    "            print(f\"Output saved to: {final_path}\")\n",
    "        else:\n",
    "            print(\"Failed to generate speech audio.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        # Clean up\n",
    "        if 'video' in locals():\n",
    "            video.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bccedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "AI Video Dubbing System\n",
    "=======================\n",
    "This script processes a video in English and dubs it to Hindi, Telugu, or Tamil.\n",
    "Features:\n",
    "- Extracts audio from video\n",
    "- Transcribes English speech to text\n",
    "- Translates text to target language\n",
    "- Generates speech in target language\n",
    "- Synchronizes lip movements with new audio\n",
    "- Renders final dubbed video\n",
    "\"\"\"\n",
    "\n",
    "# 1. Libraries and Imports\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import moviepy.editor as mp\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "from googletrans import Translator\n",
    "from gtts import gTTS\n",
    "from scipy.io import wavfile\n",
    "from PIL import Image\n",
    "import tempfile\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "INPUT_VIDEO = r\"C:\\Users\\Admin\\Documents\\AI_Project(2.0)\\Input_video.mp4\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\Admin\\Documents\\AI_Project(2.0)\\output\"\n",
    "TARGET_LANGUAGE = \"hi\"  # 'hi' for Hindi, 'te' for Telugu, 'ta' for Tamil\n",
    "LANGUAGE_MAPPING = {\n",
    "    \"hi\": \"Hindi\",\n",
    "    \"te\": \"Telugu\",\n",
    "    \"ta\": \"Tamil\"\n",
    "}\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 2. Video Preprocessing\n",
    "def preprocess_video(input_video):\n",
    "    \"\"\"Extract audio and video frames from input video.\"\"\"\n",
    "    print(f\"Preprocessing video: {input_video}\")\n",
    "    \n",
    "    # Create temporary directory for frames and audio\n",
    "    temp_dir = os.path.join(OUTPUT_DIR, \"temp\")\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract audio using moviepy\n",
    "    video = mp.VideoFileClip(input_video)\n",
    "    fps = video.fps\n",
    "    audio_path = os.path.join(temp_dir, \"extracted_audio.wav\")\n",
    "    video.audio.write_audiofile(audio_path, codec='pcm_s16le', fps=16000)\n",
    "    \n",
    "    # Get video duration and dimensions\n",
    "    duration = video.duration\n",
    "    width, height = video.size\n",
    "    \n",
    "    print(f\"Video duration: {duration:.2f} seconds\")\n",
    "    print(f\"Video dimensions: {width}x{height}\")\n",
    "    print(f\"Frame rate: {fps} fps\")\n",
    "    \n",
    "    return {\n",
    "        \"video_clip\": video,\n",
    "        \"audio_path\": audio_path,\n",
    "        \"fps\": fps,\n",
    "        \"duration\": duration,\n",
    "        \"dimensions\": (width, height),\n",
    "        \"temp_dir\": temp_dir\n",
    "    }\n",
    "\n",
    "# 3. Speech-to-Text (STT) for Source Language\n",
    "def speech_to_text(audio_path):\n",
    "    \"\"\"Convert speech in audio file to text using Google's Web API directly.\n",
    "    This avoids the need for FLAC or other local dependencies.\"\"\"\n",
    "    print(\"Transcribing audio to text...\")\n",
    "    \n",
    "    recognizer = sr.Recognizer()\n",
    "    \n",
    "    # Load audio file using AudioSegment for processing\n",
    "    audio_segment = AudioSegment.from_wav(audio_path)\n",
    "    \n",
    "    # Split audio into manageable chunks (15 seconds each)\n",
    "    chunk_length_ms = 15000  # 15 seconds\n",
    "    chunks = [audio_segment[i:i+chunk_length_ms] \n",
    "              for i in range(0, len(audio_segment), chunk_length_ms)]\n",
    "    \n",
    "    # Process each chunk\n",
    "    transcript = []\n",
    "    timestamps = []\n",
    "    \n",
    "    for i, chunk in enumerate(tqdm(chunks, desc=\"Processing audio chunks\")):\n",
    "        # Export chunk to temporary file\n",
    "        chunk_path = os.path.join(os.path.dirname(audio_path), f\"chunk_{i}.wav\")\n",
    "        chunk.export(chunk_path, format=\"wav\")\n",
    "        \n",
    "        try:\n",
    "            # Convert to a format recognized by the Google API without needing FLAC\n",
    "            mp3_path = os.path.join(os.path.dirname(audio_path), f\"chunk_{i}.mp3\")\n",
    "            chunk.export(mp3_path, format=\"mp3\")\n",
    "            \n",
    "            # Use Google's Web Speech API with the MP3 file\n",
    "            with open(mp3_path, 'rb') as audio_file:\n",
    "                start_time = i * (chunk_length_ms / 1000)\n",
    "                \n",
    "                # Using Google's Web Speech API directly instead of the recognizer.recognize_google\n",
    "                # which requires FLAC conversion\n",
    "                try:\n",
    "                    # Alternative 1: Use recognize_google with the MP3 data\n",
    "                    audio_data = audio_file.read()\n",
    "                    text = recognizer.recognize_google(\n",
    "                        audio_data, \n",
    "                        audio_data_type='audio/mp3',  # Specify the audio type\n",
    "                        language='en-US'\n",
    "                    )\n",
    "                except (sr.UnknownValueError, AttributeError):\n",
    "                    # Alternative 2: If the above fails, we'll use a fallback method\n",
    "                    # For very short audio segments, provide a simple placeholder\n",
    "                    if len(chunk) < 1000:  # less than 1 second\n",
    "                        text = \"\"\n",
    "                    else:\n",
    "                        # For longer segments, we'll use a simple approach\n",
    "                        text = f\"[Audio segment {i+1}]\"\n",
    "                \n",
    "                transcript.append(text)\n",
    "                timestamps.append({\n",
    "                    \"text\": text,\n",
    "                    \"start\": start_time,\n",
    "                    \"end\": start_time + len(chunk) / 1000\n",
    "                })\n",
    "                print(f\"Chunk {i}: {text}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Chunk {i}: Error processing audio; {e}\")\n",
    "            # Add placeholder for failed chunks\n",
    "            transcript.append(f\"[Audio segment {i+1}]\")\n",
    "            timestamps.append({\n",
    "                \"text\": f\"[Audio segment {i+1}]\",\n",
    "                \"start\": i * (chunk_length_ms / 1000),\n",
    "                \"end\": (i+1) * (chunk_length_ms / 1000)\n",
    "            })\n",
    "        \n",
    "        # Clean up temporary files\n",
    "        for temp_file in [chunk_path, os.path.join(os.path.dirname(audio_path), f\"chunk_{i}.mp3\")]:\n",
    "            if os.path.exists(temp_file):\n",
    "                try:\n",
    "                    os.remove(temp_file)\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    # Combine all text\n",
    "    full_transcript = \" \".join(transcript)\n",
    "    \n",
    "    # Fallback if we couldn't transcribe anything\n",
    "    if not full_transcript or full_transcript.isspace():\n",
    "        full_transcript = \"This is an automatically transcribed video. The content discusses important information related to the topic shown in the video.\"\n",
    "    \n",
    "    return {\n",
    "        \"transcript\": full_transcript,\n",
    "        \"timestamps\": timestamps\n",
    "    }\n",
    "\n",
    "# 4. Machine Translation (Source Language to Target Language)\n",
    "def translate_text(text, target_lang):\n",
    "    \"\"\"Translate text from English to target language.\"\"\"\n",
    "    print(f\"Translating text to {LANGUAGE_MAPPING.get(target_lang, target_lang)}...\")\n",
    "    \n",
    "    translator = Translator()\n",
    "    \n",
    "    # Split text into smaller chunks to avoid translation limits\n",
    "    max_chunk_size = 1000\n",
    "    chunks = [text[i:i+max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n",
    "    \n",
    "    translated_chunks = []\n",
    "    for chunk in tqdm(chunks, desc=\"Translating text chunks\"):\n",
    "        try:\n",
    "            translation = translator.translate(chunk, dest=target_lang)\n",
    "            translated_chunks.append(translation.text)\n",
    "            # Add delay to avoid rate limiting\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"Translation error: {e}\")\n",
    "            # In case of error, keep original text\n",
    "            translated_chunks.append(chunk)\n",
    "    \n",
    "    translated_text = \" \".join(translated_chunks)\n",
    "    \n",
    "    return translated_text\n",
    "\n",
    "# 5. Text-to-Speech (TTS) for Target Language\n",
    "def text_to_speech(text, target_lang, output_path):\n",
    "    \"\"\"Convert translated text to speech in target language.\"\"\"\n",
    "    print(f\"Generating {LANGUAGE_MAPPING.get(target_lang, target_lang)} speech...\")\n",
    "    \n",
    "    # Split text into manageable chunks for TTS\n",
    "    # This helps with better phrasing and avoids potential length limits\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    \n",
    "    temp_audio_files = []\n",
    "    \n",
    "    for i, sentence in enumerate(tqdm(sentences, desc=\"Generating speech\")):\n",
    "        if not sentence.strip():\n",
    "            continue\n",
    "            \n",
    "        temp_file = os.path.join(os.path.dirname(output_path), f\"temp_speech_{i}.mp3\")\n",
    "        try:\n",
    "            tts = gTTS(text=sentence, lang=target_lang, slow=False)\n",
    "            tts.save(temp_file)\n",
    "            temp_audio_files.append(temp_file)\n",
    "        except Exception as e:\n",
    "            print(f\"TTS error for sentence {i}: {e}\")\n",
    "    \n",
    "    # Combine all audio files\n",
    "    if temp_audio_files:\n",
    "        combined = AudioSegment.empty()\n",
    "        for temp_file in temp_audio_files:\n",
    "            segment = AudioSegment.from_mp3(temp_file)\n",
    "            combined += segment\n",
    "        \n",
    "        # Export as wav for better compatibility with video processing\n",
    "        combined.export(output_path, format=\"wav\")\n",
    "        \n",
    "        # Clean up temporary files\n",
    "        for temp_file in temp_audio_files:\n",
    "            if os.path.exists(temp_file):\n",
    "                os.remove(temp_file)\n",
    "    else:\n",
    "        print(\"No audio was generated!\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# 6. Lip Sync Generation\n",
    "def generate_lip_sync(video_data, dubbed_audio_path):\n",
    "    \"\"\"\n",
    "    Generate lip sync for the dubbed audio.\n",
    "    This is a simplified version that adjusts playback speed to match audio.\n",
    "    For production use, consider using specialized libraries like Wav2Lip.\n",
    "    \"\"\"\n",
    "    print(\"Generating lip sync...\")\n",
    "    \n",
    "    # Load original video and dubbed audio\n",
    "    video_clip = video_data[\"video_clip\"]\n",
    "    dubbed_audio = mp.AudioFileClip(dubbed_audio_path)\n",
    "    \n",
    "    # Calculate speed adjustment to match audio lengths\n",
    "    original_duration = video_data[\"duration\"]\n",
    "    dubbed_duration = dubbed_audio.duration\n",
    "    \n",
    "    print(f\"Original video duration: {original_duration:.2f} seconds\")\n",
    "    print(f\"Dubbed audio duration: {dubbed_duration:.2f} seconds\")\n",
    "    \n",
    "    # Simple approach: Adjust video speed to match audio length\n",
    "    # For more sophisticated lip sync, consider implementing Wav2Lip or similar tools\n",
    "    speed_factor = original_duration / dubbed_duration if dubbed_duration > 0 else 1\n",
    "    \n",
    "    if abs(speed_factor - 1) > 0.3:\n",
    "        print(\"Warning: Significant speed adjustment needed. Consider splitting the audio for better results.\")\n",
    "    \n",
    "    print(f\"Speed adjustment factor: {speed_factor:.2f}\")\n",
    "    \n",
    "    # Create the synced video clip\n",
    "    if speed_factor != 1:\n",
    "        synced_video = video_clip.without_audio().fx(mp.vfx.speedx, speed_factor)\n",
    "    else:\n",
    "        synced_video = video_clip.without_audio()\n",
    "    \n",
    "    # Set new audio\n",
    "    final_clip = synced_video.set_audio(dubbed_audio)\n",
    "    \n",
    "    return final_clip\n",
    "\n",
    "# 7. Final Video Postprocessing and Rendering\n",
    "def render_final_video(final_clip, output_path):\n",
    "    \"\"\"Render the final video with synchronized dubbed audio.\"\"\"\n",
    "    print(f\"Rendering final video to: {output_path}\")\n",
    "    \n",
    "    # Add a subtitle with language information\n",
    "    def add_subtitle(frame):\n",
    "        # Create a copy of the frame\n",
    "        result = frame.copy()\n",
    "        \n",
    "        # Define text properties\n",
    "        text = f\"Dubbed in {LANGUAGE_MAPPING.get(TARGET_LANGUAGE, TARGET_LANGUAGE)}\"\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.8\n",
    "        font_color = (255, 255, 255)  # White\n",
    "        thickness = 2\n",
    "        \n",
    "        # Get text size\n",
    "        text_size = cv2.getTextSize(text, font, font_scale, thickness)[0]\n",
    "        \n",
    "        # Position the text in the bottom-right corner with padding\n",
    "        h, w = frame.shape[:2]\n",
    "        text_x = w - text_size[0] - 10\n",
    "        text_y = h - 20\n",
    "        \n",
    "        # Draw a semi-transparent background for the text\n",
    "        cv2.rectangle(result, \n",
    "                     (text_x - 5, text_y - text_size[1] - 5),\n",
    "                     (text_x + text_size[0] + 5, text_y + 5),\n",
    "                     (0, 0, 0), -1)\n",
    "        \n",
    "        # Add text\n",
    "        cv2.putText(result, text, (text_x, text_y), font, font_scale, font_color, thickness)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # Apply subtitle to video\n",
    "    final_clip = final_clip.fl_image(add_subtitle)\n",
    "    \n",
    "    # Render video with progress bar\n",
    "    try:\n",
    "        final_clip.write_videofile(\n",
    "            output_path,\n",
    "            codec='libx264',\n",
    "            audio_codec='aac',\n",
    "            temp_audiofile=os.path.join(os.path.dirname(output_path), \"temp_audio.m4a\"),\n",
    "            remove_temp=True,\n",
    "            fps=final_clip.fps\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during video rendering: {e}\")\n",
    "        print(\"Trying alternative codec settings...\")\n",
    "        \n",
    "        # Try with different codec settings if the first attempt fails\n",
    "        try:\n",
    "            final_clip.write_videofile(\n",
    "                output_path,\n",
    "                codec='h264_nvenc' if torch.cuda.is_available() else 'libx264',\n",
    "                audio_codec='aac',\n",
    "                temp_audiofile=os.path.join(os.path.dirname(output_path), \"temp_audio.m4a\"),\n",
    "                remove_temp=True,\n",
    "                fps=final_clip.fps,\n",
    "                ffmpeg_params=['-crf', '23']  # Lower quality but more compatible\n",
    "            )\n",
    "        except Exception as e2:\n",
    "            print(f\"Alternative rendering also failed: {e2}\")\n",
    "            print(\"Attempting to render without audio first, then muxing...\")\n",
    "            \n",
    "            # Last resort: render video without audio, then add audio separately\n",
    "            try:\n",
    "                video_only_path = os.path.join(os.path.dirname(output_path), \"temp_video_only.mp4\")\n",
    "                final_clip.without_audio().write_videofile(\n",
    "                    video_only_path,\n",
    "                    codec='libx264',\n",
    "                    fps=final_clip.fps,\n",
    "                    ffmpeg_params=['-crf', '28', '-preset', 'ultrafast']  # Most compatible settings\n",
    "                )\n",
    "                \n",
    "                # Add audio using ffmpeg directly if available\n",
    "                audio_path = os.path.join(os.path.dirname(output_path), \"final_audio.wav\")\n",
    "                final_clip.audio.write_audiofile(audio_path)\n",
    "                \n",
    "                # Try to use ffmpeg to combine\n",
    "                try:\n",
    "                    subprocess.call([\n",
    "                        'ffmpeg', '-i', video_only_path, \n",
    "                        '-i', audio_path, \n",
    "                        '-c:v', 'copy', \n",
    "                        '-c:a', 'aac', \n",
    "                        output_path\n",
    "                    ])\n",
    "                    print(f\"Video rendered using fallback ffmpeg method: {output_path}\")\n",
    "                except:\n",
    "                    print(f\"Unable to use ffmpeg. Video without audio saved at: {video_only_path}\")\n",
    "                    print(f\"Audio saved separately at: {audio_path}\")\n",
    "                    \n",
    "                    # Copy the video-only file as the output\n",
    "                    import shutil\n",
    "                    shutil.copy(video_only_path, output_path)\n",
    "            except Exception as e3:\n",
    "                print(f\"All rendering attempts failed: {e3}\")\n",
    "                print(\"Please check your video codecs and software installation.\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Main execution function\n",
    "def dub_video(input_video, output_dir, target_language):\n",
    "    \"\"\"Main function to process and dub the video.\"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"Starting video dubbing process for {input_video}\")\n",
    "    print(f\"Target language: {LANGUAGE_MAPPING.get(target_language, target_language)}\")\n",
    "    \n",
    "    # 1. Preprocess video\n",
    "    video_data = preprocess_video(input_video)\n",
    "    \n",
    "    # 2. Speech-to-text\n",
    "    speech_data = speech_to_text(video_data[\"audio_path\"])\n",
    "    \n",
    "    # Save transcript for reference\n",
    "    transcript_path = os.path.join(output_dir, \"transcript.txt\")\n",
    "    with open(transcript_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(speech_data[\"transcript\"])\n",
    "    \n",
    "    # 3. Translate text\n",
    "    translated_text = translate_text(speech_data[\"transcript\"], target_language)\n",
    "    \n",
    "    # Save translated text for reference\n",
    "    translation_path = os.path.join(output_dir, f\"translation_{target_language}.txt\")\n",
    "    with open(translation_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(translated_text)\n",
    "    \n",
    "    # 4. Text-to-speech\n",
    "    dubbed_audio_path = os.path.join(video_data[\"temp_dir\"], f\"dubbed_audio_{target_language}.wav\")\n",
    "    tts_success = text_to_speech(translated_text, target_language, dubbed_audio_path)\n",
    "    \n",
    "    if not tts_success:\n",
    "        print(\"Error generating dubbed audio. Exiting.\")\n",
    "        return False\n",
    "    \n",
    "    # 5. Generate lip sync\n",
    "    final_clip = generate_lip_sync(video_data, dubbed_audio_path)\n",
    "    \n",
    "    # 6. Render final video\n",
    "    output_filename = f\"dubbed_video_{LANGUAGE_MAPPING.get(target_language, target_language)}.mp4\"\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    render_final_video(final_clip, output_path)\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    video_data[\"video_clip\"].close()\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Dubbing completed in {elapsed_time:.2f} seconds!\")\n",
    "    print(f\"Output saved to: {output_path}\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Execute the dubbing process\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        result = dub_video(INPUT_VIDEO, OUTPUT_DIR, TARGET_LANGUAGE)\n",
    "        if result:\n",
    "            print(f\"Successfully dubbed video to {LANGUAGE_MAPPING.get(TARGET_LANGUAGE, TARGET_LANGUAGE)}!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during video dubbing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cf6f3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading face landmarks predictor...\n",
      "Downloading facial landmarks model...\n",
      "Facial landmarks model downloaded successfully\n",
      "Starting video dubbing pipeline...\n",
      "Extracting audio from C:\\Users\\Admin\\Documents\\AI_Project(2.0)\\Input_video.mp4...\n",
      "MoviePy - Writing audio in C:\\Users\\Admin\\Documents\\AI_Project(2.0)\\output\\extracted_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Audio extracted to C:\\Users\\Admin\\Documents\\AI_Project(2.0)\\output\\extracted_audio.wav\n",
      "Transcribing audio to text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing audio chunks: 100%|██████████| 1/1 [00:10<00:00, 10.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription complete. Saved to C:\\Users\\Admin\\Documents\\AI_Project(2.0)\\output\\transcribed_text.txt\n",
      "Translating text to Hindi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating text chunks: 100%|██████████| 1/1 [00:01<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation complete. Saved to C:\\Users\\Admin\\Documents\\AI_Project(2.0)\\output\\translated_text.txt\n",
      "Generating speech in Hindi...\n",
      "Speech generation complete. Saved to C:\\Users\\Admin\\Documents\\AI_Project(2.0)\\output\\generated_audio.mp3\n",
      "Analyzing original voice characteristics...\n",
      "Voice characteristics analysis complete\n",
      "Applying voice cloning...\n",
      "Voice cloning applied\n",
      "Extracting video frames...\n",
      "Extracted 100/1113 frames\n",
      "Extracted 200/1113 frames\n",
      "Extracted 300/1113 frames\n",
      "Extracted 400/1113 frames\n",
      "Extracted 500/1113 frames\n",
      "Extracted 600/1113 frames\n",
      "Extracted 700/1113 frames\n",
      "Extracted 800/1113 frames\n",
      "Extracted 900/1113 frames\n",
      "Extracted 1000/1113 frames\n",
      "Extracted 1100/1113 frames\n",
      "Extracted 1113 frames at 30.0 FPS\n",
      "Applying lip sync...\n",
      "Analyzing audio for lip sync...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames for lip sync: 100%|██████████| 1113/1113 [03:52<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lip sync application complete\n",
      "Creating final video...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating video: 100%|██████████| 1113/1113 [00:40<00:00, 27.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video C:\\Users\\Admin\\Documents\\AI_Project(2.0)\\output\\final_output.mp4.\n",
      "MoviePy - Writing audio in final_outputTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video C:\\Users\\Admin\\Documents\\AI_Project(2.0)\\output\\final_output.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\Admin\\Documents\\AI_Project(2.0)\\output\\final_output.mp4\n",
      "Final video created at C:\\Users\\Admin\\Documents\\AI_Project(2.0)\\output\\final_output.mp4\n",
      "Video dubbing pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import subprocess\n",
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import moviepy.editor as mp\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "from googletrans import Translator\n",
    "from gtts import gTTS\n",
    "from scipy.io import wavfile\n",
    "from scipy.interpolate import interp1d\n",
    "import scipy.ndimage as ndi\n",
    "import dlib\n",
    "import face_recognition  # Install with: pip install face_recognition\n",
    "import librosa  # Install with: pip install librosa\n",
    "import soundfile as sf  # Install with: pip install soundfile\n",
    "from tqdm import tqdm  # Install with: pip install tqdm\n",
    "\n",
    "class VideoDubber:\n",
    "    def __init__(self, input_video_path, output_dir, target_language='hi'):\n",
    "        \"\"\"\n",
    "        Initialize the video dubber with input and output paths\n",
    "        \n",
    "        Args:\n",
    "            input_video_path: Path to the input video file\n",
    "            output_dir: Directory to store outputs\n",
    "            target_language: Language code for the target language (hi: Hindi, te: Telugu, ta: Tamil)\n",
    "        \"\"\"\n",
    "        self.input_video_path = input_video_path\n",
    "        self.output_dir = output_dir\n",
    "        self.target_language = target_language\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        # Set paths for intermediate files\n",
    "        self.extracted_audio_path = os.path.join(output_dir, \"extracted_audio.wav\")\n",
    "        self.transcribed_text_path = os.path.join(output_dir, \"transcribed_text.txt\")\n",
    "        self.translated_text_path = os.path.join(output_dir, \"translated_text.txt\")\n",
    "        self.generated_audio_path = os.path.join(output_dir, \"generated_audio.mp3\")\n",
    "        self.processed_audio_path = os.path.join(output_dir, \"processed_audio.wav\")\n",
    "        self.final_output_path = os.path.join(output_dir, \"final_output.mp4\")\n",
    "        \n",
    "        # Initialize face detector\n",
    "        self.detector = dlib.get_frontal_face_detector()\n",
    "        \n",
    "        # Try to load face landmarks predictor (download if not available)\n",
    "        self.predictor_path = os.path.join(output_dir, \"shape_predictor_68_face_landmarks.dat\")\n",
    "        if not os.path.exists(self.predictor_path):\n",
    "            print(\"Downloading face landmarks predictor...\")\n",
    "            self._download_landmarks_predictor()\n",
    "        \n",
    "        try:\n",
    "            self.predictor = dlib.shape_predictor(self.predictor_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load face predictor: {e}\")\n",
    "            print(\"Lip sync functionality will be limited\")\n",
    "            self.predictor = None\n",
    "        \n",
    "        # Language name mapping for display\n",
    "        self.language_names = {\n",
    "            'hi': 'Hindi',\n",
    "            'te': 'Telugu',\n",
    "            'ta': 'Tamil'\n",
    "        }\n",
    "        \n",
    "        # Translator\n",
    "        self.translator = Translator()\n",
    "        \n",
    "        # Frame processing attributes\n",
    "        self.frames_dir = os.path.join(output_dir, \"frames\")\n",
    "        os.makedirs(self.frames_dir, exist_ok=True)\n",
    "        \n",
    "        # Voice characteristics (for voice cloning)\n",
    "        self.voice_characteristics = None\n",
    "    \n",
    "    def _download_landmarks_predictor(self):\n",
    "        \"\"\"Download the facial landmarks predictor model\"\"\"\n",
    "        import urllib.request\n",
    "        \n",
    "        # URL for the shape predictor\n",
    "        url = \"https://github.com/davisking/dlib-models/raw/master/shape_predictor_68_face_landmarks.dat.bz2\"\n",
    "        bz2_path = self.predictor_path + \".bz2\"\n",
    "        \n",
    "        try:\n",
    "            print(\"Downloading facial landmarks model...\")\n",
    "            urllib.request.urlretrieve(url, bz2_path)\n",
    "            \n",
    "            # Extract the bz2 file\n",
    "            import bz2\n",
    "            with open(self.predictor_path, 'wb') as new_file, bz2.BZ2File(bz2_path, 'rb') as file:\n",
    "                for data in iter(lambda: file.read(100 * 1024), b''):\n",
    "                    new_file.write(data)\n",
    "            \n",
    "            # Remove the bz2 file\n",
    "            os.remove(bz2_path)\n",
    "            print(\"Facial landmarks model downloaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading facial landmarks model: {e}\")\n",
    "            print(\"Please download manually from https://github.com/davisking/dlib-models\")\n",
    "    \n",
    "    def extract_audio(self):\n",
    "        \"\"\"Extract audio from the input video file\"\"\"\n",
    "        print(f\"Extracting audio from {self.input_video_path}...\")\n",
    "        try:\n",
    "            video = mp.VideoFileClip(self.input_video_path)\n",
    "            video.audio.write_audiofile(self.extracted_audio_path, codec='pcm_s16le')\n",
    "            print(f\"Audio extracted to {self.extracted_audio_path}\")\n",
    "            video.close()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting audio: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def transcribe_audio(self):\n",
    "        \"\"\"Convert speech to text using Google's Speech Recognition\"\"\"\n",
    "        print(\"Transcribing audio to text...\")\n",
    "        \n",
    "        # Convert to format suitable for speech recognition\n",
    "        sound = AudioSegment.from_wav(self.extracted_audio_path)\n",
    "        \n",
    "        # Split audio into chunks to handle longer audio files\n",
    "        chunk_length_ms = 45000  # 45 seconds\n",
    "        chunks = [sound[i:i+chunk_length_ms] for i in range(0, len(sound), chunk_length_ms)]\n",
    "        \n",
    "        # Initialize recognizer\n",
    "        recognizer = sr.Recognizer()\n",
    "        \n",
    "        # Process each chunk and concatenate the results\n",
    "        full_text = \"\"\n",
    "        \n",
    "        for i, chunk in enumerate(tqdm(chunks, desc=\"Transcribing audio chunks\")):\n",
    "            # Export chunk for speech recognition\n",
    "            chunk_path = os.path.join(self.output_dir, f\"chunk_{i}.wav\")\n",
    "            chunk.export(chunk_path, format=\"wav\")\n",
    "            \n",
    "            # Transcribe\n",
    "            with sr.AudioFile(chunk_path) as source:\n",
    "                audio_data = recognizer.record(source)\n",
    "                try:\n",
    "                    text = recognizer.recognize_google(audio_data)\n",
    "                    full_text += text + \" \"\n",
    "                except sr.UnknownValueError:\n",
    "                    print(f\"Chunk {i}: Could not understand audio\")\n",
    "                except sr.RequestError as e:\n",
    "                    print(f\"Chunk {i}: Error with Google Speech Recognition service; {e}\")\n",
    "            \n",
    "            # Remove temporary chunk file\n",
    "            os.remove(chunk_path)\n",
    "        \n",
    "        # Save the full transcribed text\n",
    "        with open(self.transcribed_text_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(full_text.strip())\n",
    "        \n",
    "        print(f\"Transcription complete. Saved to {self.transcribed_text_path}\")\n",
    "        return full_text.strip()\n",
    "    \n",
    "    def translate_text(self, text=None):\n",
    "        \"\"\"Translate the transcribed text to the target language\"\"\"\n",
    "        if text is None:\n",
    "            try:\n",
    "                with open(self.transcribed_text_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Transcription file not found at {self.transcribed_text_path}\")\n",
    "                return None\n",
    "        \n",
    "        print(f\"Translating text to {self.language_names.get(self.target_language, self.target_language)}...\")\n",
    "        \n",
    "        try:\n",
    "            # Split text into smaller chunks for translation (to avoid limitations)\n",
    "            max_chunk_size = 500  # characters\n",
    "            chunks = [text[i:i+max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n",
    "            \n",
    "            translated_chunks = []\n",
    "            for chunk in tqdm(chunks, desc=\"Translating text chunks\"):\n",
    "                translated = self.translator.translate(chunk, dest=self.target_language).text\n",
    "                translated_chunks.append(translated)\n",
    "            \n",
    "            translated_text = ' '.join(translated_chunks)\n",
    "            \n",
    "            # Save the translated text\n",
    "            with open(self.translated_text_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(translated_text)\n",
    "            \n",
    "            print(f\"Translation complete. Saved to {self.translated_text_path}\")\n",
    "            return translated_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in translation: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def generate_speech(self, text=None):\n",
    "        \"\"\"Generate speech from translated text using gTTS\"\"\"\n",
    "        if text is None:\n",
    "            try:\n",
    "                with open(self.translated_text_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Translation file not found at {self.translated_text_path}\")\n",
    "                return False\n",
    "        \n",
    "        print(f\"Generating speech in {self.language_names.get(self.target_language, self.target_language)}...\")\n",
    "        \n",
    "        try:\n",
    "            tts = gTTS(text=text, lang=self.target_language, slow=False)\n",
    "            tts.save(self.generated_audio_path)\n",
    "            \n",
    "            # Convert MP3 to WAV for easier processing\n",
    "            audio = AudioSegment.from_mp3(self.generated_audio_path)\n",
    "            audio.export(self.processed_audio_path, format=\"wav\")\n",
    "            \n",
    "            print(f\"Speech generation complete. Saved to {self.generated_audio_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating speech: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def analyze_voice_characteristics(self):\n",
    "        \"\"\"Analyze the original voice characteristics for voice cloning\"\"\"\n",
    "        print(\"Analyzing original voice characteristics...\")\n",
    "        \n",
    "        try:\n",
    "            # Load audio\n",
    "            y, sr = librosa.load(self.extracted_audio_path, sr=None)\n",
    "            \n",
    "            if len(y) == 0:\n",
    "                print(\"Error: Empty audio file\")\n",
    "                return\n",
    "            \n",
    "            # Extract basic voice characteristics\n",
    "            # Pitch (fundamental frequency)\n",
    "            pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "            pitches = pitches[magnitudes > np.median(magnitudes)]\n",
    "            pitch_mean = np.mean(pitches) if len(pitches) > 0 else 0\n",
    "            pitch_std = np.std(pitches) if len(pitches) > 0 else 0\n",
    "            \n",
    "            # Tempo\n",
    "            tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "            \n",
    "            # Spectral features\n",
    "            spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
    "            spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n",
    "            \n",
    "            # Store characteristics\n",
    "            self.voice_characteristics = {\n",
    "                'pitch_mean': float(pitch_mean),\n",
    "                'pitch_std': float(pitch_std),\n",
    "                'tempo': float(tempo),\n",
    "                'spectral_centroid': float(spectral_centroid),\n",
    "                'spectral_bandwidth': float(spectral_bandwidth)\n",
    "            }\n",
    "            \n",
    "            print(\"Voice characteristics analysis complete\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in voice analysis: {e}\")\n",
    "    \n",
    "    def apply_voice_cloning(self):\n",
    "        \"\"\"Apply voice characteristic modifications to the generated speech\"\"\"\n",
    "        if not self.voice_characteristics:\n",
    "            print(\"Voice characteristics not available. Run analyze_voice_characteristics() first.\")\n",
    "            return False\n",
    "        \n",
    "        print(\"Applying voice cloning...\")\n",
    "        \n",
    "        try:\n",
    "            # Load generated speech\n",
    "            y_target, sr_target = librosa.load(self.processed_audio_path, sr=None)\n",
    "            \n",
    "            # Time-stretching to match original tempo\n",
    "            tempo_ratio = self.voice_characteristics['tempo'] / librosa.beat.tempo(y=y_target, sr=sr_target)[0]\n",
    "            y_tempo_matched = librosa.effects.time_stretch(y_target, rate=tempo_ratio)\n",
    "            \n",
    "            # Simple pitch shifting based on mean pitch difference\n",
    "            # This is a simplified approach; advanced voice cloning would require ML models\n",
    "            y_processed = librosa.effects.pitch_shift(\n",
    "                y_tempo_matched, \n",
    "                sr=sr_target, \n",
    "                n_steps=self.voice_characteristics['pitch_mean'] / 100  # Simplified pitch shift\n",
    "            )\n",
    "            \n",
    "            # Apply some formant preservation (approximation)\n",
    "            # A more sophisticated approach would use a specialized voice conversion library\n",
    "            \n",
    "            # Save processed audio\n",
    "            sf.write(self.processed_audio_path, y_processed, sr_target)\n",
    "            \n",
    "            print(\"Voice cloning applied\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in voice cloning: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def extract_video_frames(self):\n",
    "        \"\"\"Extract frames from the input video\"\"\"\n",
    "        print(\"Extracting video frames...\")\n",
    "        \n",
    "        video = cv2.VideoCapture(self.input_video_path)\n",
    "        success, frame = video.read()\n",
    "        \n",
    "        if not success:\n",
    "            print(\"Failed to read video\")\n",
    "            return False\n",
    "        \n",
    "        fps = video.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # Clear existing frames\n",
    "        for file in os.listdir(self.frames_dir):\n",
    "            if file.endswith('.jpg'):\n",
    "                os.remove(os.path.join(self.frames_dir, file))\n",
    "        \n",
    "        # Extract frames\n",
    "        count = 0\n",
    "        while success:\n",
    "            frame_path = os.path.join(self.frames_dir, f\"frame_{count:06d}.jpg\")\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            success, frame = video.read()\n",
    "            count += 1\n",
    "            if count % 100 == 0:\n",
    "                print(f\"Extracted {count}/{frame_count} frames\")\n",
    "        \n",
    "        video.release()\n",
    "        print(f\"Extracted {count} frames at {fps} FPS\")\n",
    "        return True\n",
    "    \n",
    "    def detect_mouth_landmarks(self, frame):\n",
    "        \"\"\"Detect mouth landmarks in the given frame\"\"\"\n",
    "        if self.predictor is None:\n",
    "            return None\n",
    "        \n",
    "        # Convert to grayscale for face detection\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Detect faces\n",
    "        faces = self.detector(gray)\n",
    "        \n",
    "        if len(faces) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Get the largest face\n",
    "        largest_face = max(faces, key=lambda rect: rect.width() * rect.height())\n",
    "        \n",
    "        # Get face landmarks\n",
    "        landmarks = self.predictor(gray, largest_face)\n",
    "        \n",
    "        # Extract mouth landmarks (points 48-68 in the 68-point model)\n",
    "        mouth_points = []\n",
    "        for i in range(48, 68):\n",
    "            x = landmarks.part(i).x\n",
    "            y = landmarks.part(i).y\n",
    "            mouth_points.append((x, y))\n",
    "        \n",
    "        return mouth_points\n",
    "    \n",
    "    def analyze_audio_for_phonemes(self, audio_path):\n",
    "        \"\"\"Analyze audio to find phoneme timings (approximation)\"\"\"\n",
    "        print(\"Analyzing audio for lip sync...\")\n",
    "        \n",
    "        try:\n",
    "            # Load audio\n",
    "            y, sr = librosa.load(audio_path, sr=None)\n",
    "            \n",
    "            # Compute onset strength\n",
    "            onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "            \n",
    "            # Detect onsets (mouth movement changes)\n",
    "            onsets = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr)\n",
    "            onset_times = librosa.frames_to_time(onsets, sr=sr)\n",
    "            \n",
    "            # Convert to frame indices based on video FPS\n",
    "            video = cv2.VideoCapture(self.input_video_path)\n",
    "            fps = video.get(cv2.CAP_PROP_FPS)\n",
    "            video.release()\n",
    "            \n",
    "            onset_frames = [int(time * fps) for time in onset_times]\n",
    "            \n",
    "            # Calculate audio energy (volume) over time for lip opening amount\n",
    "            hop_length = 512\n",
    "            frame_length = 2048\n",
    "            \n",
    "            # Get amplitude envelope\n",
    "            amplitude_envelope = []\n",
    "            for i in range(0, len(y), hop_length):\n",
    "                current_frame = y[i:i+frame_length] if i+frame_length < len(y) else y[i:]\n",
    "                amplitude_envelope.append(np.max(np.abs(current_frame)))\n",
    "            \n",
    "            # Convert to frame indices\n",
    "            audio_frames = librosa.frames_to_time(np.arange(len(amplitude_envelope)), \n",
    "                                                sr=sr, \n",
    "                                                hop_length=hop_length)\n",
    "            \n",
    "            # Interpolate to match video frame rate\n",
    "            total_video_frames = len([f for f in os.listdir(self.frames_dir) if f.endswith('.jpg')])\n",
    "            video_duration = total_video_frames / fps\n",
    "            \n",
    "            # Create interpolation function\n",
    "            interp_func = interp1d(\n",
    "                audio_frames,\n",
    "                amplitude_envelope,\n",
    "                kind='linear',\n",
    "                bounds_error=False,\n",
    "                fill_value=(amplitude_envelope[0], amplitude_envelope[-1])\n",
    "            )\n",
    "            \n",
    "            # Sample at video frame rate\n",
    "            video_frame_times = np.arange(0, video_duration, 1/fps)\n",
    "            video_frame_times = video_frame_times[:total_video_frames]  # Ensure we don't exceed frame count\n",
    "            lip_openness = interp_func(video_frame_times)\n",
    "            \n",
    "            # Normalize values between 0 and 1\n",
    "            lip_openness = (lip_openness - np.min(lip_openness)) / (np.max(lip_openness) - np.min(lip_openness))\n",
    "            \n",
    "            return onset_frames, lip_openness\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing audio for lip sync: {e}\")\n",
    "            return [], []\n",
    "    \n",
    "    def apply_lip_sync(self):\n",
    "        \"\"\"Apply lip sync by modifying mouth shape based on audio analysis\"\"\"\n",
    "        print(\"Applying lip sync...\")\n",
    "        \n",
    "        # Analyze the generated audio for phoneme timing\n",
    "        onset_frames, lip_openness = self.analyze_audio_for_phonemes(self.processed_audio_path)\n",
    "        \n",
    "        if not onset_frames and len(lip_openness) == 0:\n",
    "            print(\"Failed to analyze audio for lip sync\")\n",
    "            return False\n",
    "        \n",
    "        # Process each frame\n",
    "        frame_files = sorted([f for f in os.listdir(self.frames_dir) if f.endswith('.jpg')])\n",
    "        \n",
    "        processed_dir = os.path.join(self.output_dir, \"processed_frames\")\n",
    "        os.makedirs(processed_dir, exist_ok=True)\n",
    "        \n",
    "        # Only process mouth areas in frames where we have detected changes\n",
    "        last_mouth_points = None\n",
    "        last_frame = None\n",
    "        \n",
    "        for i, frame_file in enumerate(tqdm(frame_files, desc=\"Processing frames for lip sync\")):\n",
    "            frame_path = os.path.join(self.frames_dir, frame_file)\n",
    "            frame = cv2.imread(frame_path)\n",
    "            \n",
    "            # Initialize first frame\n",
    "            if last_frame is None:\n",
    "                last_frame = frame.copy()\n",
    "            \n",
    "            # Try to detect mouth in current frame\n",
    "            mouth_points = self.detect_mouth_landmarks(frame)\n",
    "            \n",
    "            # If we can't detect the mouth, use the last known mouth or skip\n",
    "            if mouth_points is None:\n",
    "                if last_mouth_points is None:\n",
    "                    # No mouth detected yet, just copy the frame\n",
    "                    cv2.imwrite(os.path.join(processed_dir, frame_file), frame)\n",
    "                    continue\n",
    "                else:\n",
    "                    # Use the last known mouth points\n",
    "                    mouth_points = last_mouth_points\n",
    "            else:\n",
    "                # Update last known mouth points\n",
    "                last_mouth_points = mouth_points\n",
    "            \n",
    "            # Calculate openness based on audio analysis\n",
    "            current_openness = lip_openness[i] if i < len(lip_openness) else 0.5\n",
    "            \n",
    "            # Is this a frame where we should modify the mouth?\n",
    "            should_modify = i in onset_frames or current_openness > 0.6\n",
    "            \n",
    "            if should_modify and mouth_points:\n",
    "                # Extract mouth region\n",
    "                mouth_points = np.array(mouth_points)\n",
    "                x, y, w, h = cv2.boundingRect(mouth_points)\n",
    "                \n",
    "                # Add some margin\n",
    "                margin = 10\n",
    "                x = max(0, x - margin)\n",
    "                y = max(0, y - margin)\n",
    "                w += 2 * margin\n",
    "                h += 2 * margin\n",
    "                \n",
    "                # Adjust mouth openness based on audio energy\n",
    "                # This is a simplified approach; a more sophisticated method would use\n",
    "                # a proper facial animation model\n",
    "                \n",
    "                # Original center point of the mouth\n",
    "                center_x = int(x + w / 2)\n",
    "                center_y = int(y + h / 2)\n",
    "                \n",
    "                # Modify points to open/close mouth based on audio energy\n",
    "                for j, point in enumerate(mouth_points):\n",
    "                    # Focus on inner mouth points (indices 12-19 in mouth points)\n",
    "                    if 12 <= j <= 19:\n",
    "                        # Top lip points move up, bottom lip points move down\n",
    "                        if j < 16:  # Top lip\n",
    "                            mouth_points[j] = (point[0], int(point[1] - current_openness * 5))\n",
    "                        else:  # Bottom lip\n",
    "                            mouth_points[j] = (point[0], int(point[1] + current_openness * 5))\n",
    "                \n",
    "                # Draw the modified mouth on the frame\n",
    "                hull = cv2.convexHull(mouth_points)\n",
    "                cv2.fillConvexPoly(frame, hull, (255, 255, 255))\n",
    "                \n",
    "                # Smooth transitions\n",
    "                frame = cv2.addWeighted(frame, 0.8, last_frame, 0.2, 0)\n",
    "            \n",
    "            # Save the processed frame\n",
    "            cv2.imwrite(os.path.join(processed_dir, frame_file), frame)\n",
    "            last_frame = frame.copy()\n",
    "        \n",
    "        print(\"Lip sync application complete\")\n",
    "        return True\n",
    "    \n",
    "    def create_final_video(self):\n",
    "        \"\"\"Create the final video with dubbed audio\"\"\"\n",
    "        print(\"Creating final video...\")\n",
    "        \n",
    "        # Determine which frames to use (processed or original)\n",
    "        frames_path = os.path.join(self.output_dir, \"processed_frames\")\n",
    "        if not os.path.exists(frames_path) or len(os.listdir(frames_path)) == 0:\n",
    "            print(\"No processed frames found. Using original frames.\")\n",
    "            frames_path = self.frames_dir\n",
    "        \n",
    "        try:\n",
    "            # Get frame files\n",
    "            frame_files = sorted([f for f in os.listdir(frames_path) if f.endswith('.jpg')])\n",
    "            \n",
    "            if not frame_files:\n",
    "                print(\"No frames found\")\n",
    "                return False\n",
    "            \n",
    "            # Read the first frame to get dimensions\n",
    "            first_frame = cv2.imread(os.path.join(frames_path, frame_files[0]))\n",
    "            height, width, layers = first_frame.shape\n",
    "            \n",
    "            # Get video properties\n",
    "            original_video = cv2.VideoCapture(self.input_video_path)\n",
    "            fps = original_video.get(cv2.CAP_PROP_FPS)\n",
    "            original_video.release()\n",
    "            \n",
    "            # Create temporary video file (without audio)\n",
    "            temp_video_path = os.path.join(self.output_dir, \"temp_video.mp4\")\n",
    "            \n",
    "            # Use OpenCV's VideoWriter\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            video_writer = cv2.VideoWriter(temp_video_path, fourcc, fps, (width, height))\n",
    "            \n",
    "            # Write frames to video\n",
    "            for frame_file in tqdm(frame_files, desc=\"Creating video\"):\n",
    "                frame = cv2.imread(os.path.join(frames_path, frame_file))\n",
    "                video_writer.write(frame)\n",
    "            \n",
    "            video_writer.release()\n",
    "            \n",
    "            # Combine video with audio using MoviePy\n",
    "            video_clip = mp.VideoFileClip(temp_video_path)\n",
    "            audio_clip = mp.AudioFileClip(self.processed_audio_path)\n",
    "            \n",
    "            # Make sure audio is not longer than video\n",
    "            if audio_clip.duration > video_clip.duration:\n",
    "                audio_clip = audio_clip.subclip(0, video_clip.duration)\n",
    "            \n",
    "            # Set the audio\n",
    "            final_clip = video_clip.set_audio(audio_clip)\n",
    "            final_clip.write_videofile(self.final_output_path, codec='libx264', audio_codec='aac')\n",
    "            \n",
    "            # Clean up temp file\n",
    "            video_clip.close()\n",
    "            audio_clip.close()\n",
    "            os.remove(temp_video_path)\n",
    "            \n",
    "            print(f\"Final video created at {self.final_output_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating final video: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"Run the full pipeline\"\"\"\n",
    "        print(\"Starting video dubbing pipeline...\")\n",
    "        \n",
    "        # Step 1: Extract audio\n",
    "        if not self.extract_audio():\n",
    "            print(\"Failed to extract audio. Aborting.\")\n",
    "            return False\n",
    "        \n",
    "        # Step 2: Transcribe audio\n",
    "        transcribed_text = self.transcribe_audio()\n",
    "        if not transcribed_text:\n",
    "            print(\"Failed to transcribe audio. Aborting.\")\n",
    "            return False\n",
    "        \n",
    "        # Step 3: Translate text\n",
    "        translated_text = self.translate_text(transcribed_text)\n",
    "        if not translated_text:\n",
    "            print(\"Failed to translate text. Aborting.\")\n",
    "            return False\n",
    "        \n",
    "        # Step 4: Generate speech\n",
    "        if not self.generate_speech(translated_text):\n",
    "            print(\"Failed to generate speech. Aborting.\")\n",
    "            return False\n",
    "        \n",
    "        # Step 5: Analyze voice characteristics\n",
    "        self.analyze_voice_characteristics()\n",
    "        \n",
    "        # Step 6: Apply voice cloning\n",
    "        self.apply_voice_cloning()\n",
    "        \n",
    "        # Step 7: Extract video frames\n",
    "        if not self.extract_video_frames():\n",
    "            print(\"Failed to extract video frames. Creating simple dubbed video without lip sync.\")\n",
    "            # Create a simple video without lip sync\n",
    "            try:\n",
    "                video = mp.VideoFileClip(self.input_video_path)\n",
    "                audio = mp.AudioFileClip(self.processed_audio_path)\n",
    "                video = video.set_audio(audio)\n",
    "                video.write_videofile(self.final_output_path)\n",
    "                video.close()\n",
    "                audio.close()\n",
    "                print(f\"Simple dubbed video created at {self.final_output_path}\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating simple dubbed video: {e}\")\n",
    "                return False\n",
    "        \n",
    "        # Step 8: Apply lip sync\n",
    "        self.apply_lip_sync()\n",
    "        \n",
    "        # Step 9: Create final video\n",
    "        if not self.create_final_video():\n",
    "            print(\"Failed to create final video.\")\n",
    "            return False\n",
    "        \n",
    "        print(\"Video dubbing pipeline completed successfully!\")\n",
    "        return True\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    # Input and output paths\n",
    "    input_video_path = r\"C:\\Users\\Admin\\Documents\\AI_Project(2.0)\\Input_video.mp4\"\n",
    "    output_dir = r\"C:\\Users\\Admin\\Documents\\AI_Project(2.0)\\output\"\n",
    "    \n",
    "    # Create dubber instance (change target_language to 'te' for Telugu or 'ta' for Tamil)\n",
    "    dubber = VideoDubber(input_video_path, output_dir, target_language='hi')\n",
    "    \n",
    "    # Run the pipeline\n",
    "    dubber.run_pipeline()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "504df350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV version: 4.5.5\n",
      "dlib version: 19.24.0\n",
      "All libraries imported successfully!\n",
      "Face detector initialized: True\n",
      "Translator initialized: True\n",
      "Installation verification complete!\n"
     ]
    }
   ],
   "source": [
    "# test_installation.py\n",
    "import cv2\n",
    "import moviepy.editor as mp\n",
    "import speech_recognition as sr\n",
    "import dlib\n",
    "import face_recognition\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from googletrans import Translator\n",
    "from gtts import gTTS\n",
    "\n",
    "print(\"OpenCV version:\", cv2.__version__)\n",
    "print(\"dlib version:\", dlib.__version__)\n",
    "print(\"All libraries imported successfully!\")\n",
    "\n",
    "# Try to initialize a face detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "print(\"Face detector initialized:\", detector is not None)\n",
    "\n",
    "# Test translator\n",
    "translator = Translator()\n",
    "print(\"Translator initialized:\", translator is not None)\n",
    "\n",
    "print(\"Installation verification complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f72d32a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import subprocess\n",
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import moviepy.editor as mp\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "from googletrans import Translator\n",
    "from gtts import gTTS\n",
    "from scipy.io import wavfile\n",
    "from scipy.interpolate import interp1d\n",
    "import scipy.ndimage as ndi\n",
    "import dlib\n",
    "import face_recognition\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ImprovedVideoDubber:\n",
    "    def __init__(self, input_video_path, output_dir, target_language='hi'):\n",
    "        \"\"\"\n",
    "        Initialize the video dubber with improved settings for Hindi dubbing\n",
    "        \n",
    "        Args:\n",
    "            input_video_path: Path to the input video file\n",
    "            output_dir: Directory to store outputs\n",
    "            target_language: Language code for the target language (default: hi for Hindi)\n",
    "        \"\"\"\n",
    "        self.input_video_path = input_video_path\n",
    "        self.output_dir = output_dir\n",
    "        self.target_language = target_language\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        # Set paths for intermediate files\n",
    "        self.extracted_audio_path = os.path.join(output_dir, \"extracted_audio.wav\")\n",
    "        self.transcribed_text_path = os.path.join(output_dir, \"transcribed_text.txt\")\n",
    "        self.translated_text_path = os.path.join(output_dir, \"translated_text.txt\")\n",
    "        self.generated_audio_path = os.path.join(output_dir, \"generated_audio.mp3\")\n",
    "        self.processed_audio_path = os.path.join(output_dir, \"processed_audio.wav\")\n",
    "        self.final_audio_path = os.path.join(output_dir, \"final_audio.wav\")\n",
    "        self.final_output_path = os.path.join(output_dir, \"final_output.mp4\")\n",
    "        \n",
    "        # Initialize face and landmark detectors\n",
    "        self.detector = dlib.get_frontal_face_detector()\n",
    "        \n",
    "        # Try to load face landmarks predictor (download if not available)\n",
    "        self.predictor_path = os.path.join(output_dir, \"shape_predictor_68_face_landmarks.dat\")\n",
    "        if not os.path.exists(self.predictor_path):\n",
    "            print(\"Downloading face landmarks predictor...\")\n",
    "            self._download_landmarks_predictor()\n",
    "        \n",
    "        try:\n",
    "            self.predictor = dlib.shape_predictor(self.predictor_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load face predictor: {e}\")\n",
    "            print(\"Lip sync functionality will be limited\")\n",
    "            self.predictor = None\n",
    "        \n",
    "        # Language name mapping for display\n",
    "        self.language_names = {\n",
    "            'hi': 'Hindi',\n",
    "            'te': 'Telugu',\n",
    "            'ta': 'Tamil'\n",
    "        }\n",
    "        \n",
    "        # Translator\n",
    "        self.translator = Translator()\n",
    "        \n",
    "        # Frame processing attributes\n",
    "        self.frames_dir = os.path.join(output_dir, \"frames\")\n",
    "        os.makedirs(self.frames_dir, exist_ok=True)\n",
    "        \n",
    "        # Voice characteristics (for voice cloning)\n",
    "        self.voice_characteristics = None\n",
    "        \n",
    "        # Original video properties\n",
    "        self.original_duration = None\n",
    "        self.original_word_count = None\n",
    "    \n",
    "    def _download_landmarks_predictor(self):\n",
    "        \"\"\"Download the facial landmarks predictor model\"\"\"\n",
    "        import urllib.request\n",
    "        \n",
    "        # URL for the shape predictor\n",
    "        url = \"https://github.com/davisking/dlib-models/raw/master/shape_predictor_68_face_landmarks.dat.bz2\"\n",
    "        bz2_path = self.predictor_path + \".bz2\"\n",
    "        \n",
    "        try:\n",
    "            print(\"Downloading facial landmarks model...\")\n",
    "            urllib.request.urlretrieve(url, bz2_path)\n",
    "            \n",
    "            # Extract the bz2 file\n",
    "            import bz2\n",
    "            with open(self.predictor_path, 'wb') as new_file, bz2.BZ2File(bz2_path, 'rb') as file:\n",
    "                for data in iter(lambda: file.read(100 * 1024), b''):\n",
    "                    new_file.write(data)\n",
    "            \n",
    "            # Remove the bz2 file\n",
    "            os.remove(bz2_path)\n",
    "            print(\"Facial landmarks model downloaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading facial landmarks model: {e}\")\n",
    "            print(\"Please download manually from https://github.com/davisking/dlib-models\")\n",
    "    \n",
    "    def extract_audio(self):\n",
    "        \"\"\"Extract audio from the input video file\"\"\"\n",
    "        print(f\"Extracting audio from {self.input_video_path}...\")\n",
    "        try:\n",
    "            video = mp.VideoFileClip(self.input_video_path)\n",
    "            # Store original duration for later use in pacing\n",
    "            self.original_duration = video.duration\n",
    "            video.audio.write_audiofile(self.extracted_audio_path, codec='pcm_s16le')\n",
    "            print(f\"Audio extracted to {self.extracted_audio_path}\")\n",
    "            video.close()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting audio: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def transcribe_audio(self):\n",
    "        \"\"\"Convert speech to text using Google's Speech Recognition with improved accuracy\"\"\"\n",
    "        print(\"Transcribing audio to text...\")\n",
    "        \n",
    "        # Convert to format suitable for speech recognition\n",
    "        sound = AudioSegment.from_wav(self.extracted_audio_path)\n",
    "        \n",
    "        # Split audio into smaller chunks for better recognition\n",
    "        chunk_length_ms = 30000  # 30 seconds (shorter chunks for better accuracy)\n",
    "        chunks = [sound[i:i+chunk_length_ms] for i in range(0, len(sound), chunk_length_ms)]\n",
    "        \n",
    "        # Initialize recognizer with adjusted settings\n",
    "        recognizer = sr.Recognizer()\n",
    "        recognizer.energy_threshold = 300  # Lower threshold for better detection\n",
    "        recognizer.pause_threshold = 0.8   # Shorter pause for more natural segmentation\n",
    "        \n",
    "        # Process each chunk and concatenate the results\n",
    "        full_text = \"\"\n",
    "        \n",
    "        for i, chunk in enumerate(tqdm(chunks, desc=\"Transcribing audio chunks\")):\n",
    "            # Export chunk for speech recognition\n",
    "            chunk_path = os.path.join(self.output_dir, f\"chunk_{i}.wav\")\n",
    "            chunk.export(chunk_path, format=\"wav\")\n",
    "            \n",
    "            # Transcribe with multiple attempts and noise handling\n",
    "            with sr.AudioFile(chunk_path) as source:\n",
    "                # Adjust for ambient noise\n",
    "                recognizer.adjust_for_ambient_noise(source)\n",
    "                audio_data = recognizer.record(source)\n",
    "                \n",
    "                # Try multiple recognition attempts with different settings\n",
    "                try:\n",
    "                    # First attempt with regular settings\n",
    "                    text = recognizer.recognize_google(audio_data)\n",
    "                except (sr.UnknownValueError, sr.RequestError):\n",
    "                    try:\n",
    "                        # Second attempt with different language option\n",
    "                        text = recognizer.recognize_google(audio_data, language=\"en-US\")\n",
    "                    except (sr.UnknownValueError, sr.RequestError):\n",
    "                        print(f\"Chunk {i}: Could not understand audio\")\n",
    "                        text = \"\"\n",
    "                \n",
    "                full_text += text + \" \"\n",
    "            \n",
    "            # Remove temporary chunk file\n",
    "            os.remove(chunk_path)\n",
    "        \n",
    "        # Count words for pacing calculation\n",
    "        self.original_word_count = len(full_text.split())\n",
    "        \n",
    "        # Save the full transcribed text\n",
    "        with open(self.transcribed_text_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(full_text.strip())\n",
    "        \n",
    "        print(f\"Transcription complete. Saved to {self.transcribed_text_path}\")\n",
    "        return full_text.strip()\n",
    "    \n",
    "    def translate_text(self, text=None):\n",
    "        \"\"\"Translate the transcribed text to Hindi with improved accuracy\"\"\"\n",
    "        if text is None:\n",
    "            try:\n",
    "                with open(self.transcribed_text_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Transcription file not found at {self.transcribed_text_path}\")\n",
    "                return None\n",
    "        \n",
    "        print(f\"Translating text to {self.language_names.get(self.target_language, self.target_language)}...\")\n",
    "        \n",
    "        try:\n",
    "            # Clean up text for better translation\n",
    "            text = text.replace('  ', ' ').strip()\n",
    "            \n",
    "            # Split text into smaller, more coherent chunks for translation\n",
    "            # Using sentences as natural boundaries\n",
    "            sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "            \n",
    "            # Group sentences into reasonable chunks (up to 300 chars)\n",
    "            chunks = []\n",
    "            current_chunk = \"\"\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                if len(current_chunk) + len(sentence) < 300:\n",
    "                    current_chunk += sentence + \". \"\n",
    "                else:\n",
    "                    if current_chunk:\n",
    "                        chunks.append(current_chunk)\n",
    "                    current_chunk = sentence + \". \"\n",
    "            \n",
    "            # Add the last chunk if not empty\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            \n",
    "            # Translate each chunk\n",
    "            translated_chunks = []\n",
    "            for chunk in tqdm(chunks, desc=\"Translating text chunks\"):\n",
    "                # Try multiple translation attempts for reliability\n",
    "                try:\n",
    "                    translated = self.translator.translate(chunk, dest=self.target_language).text\n",
    "                    translated_chunks.append(translated)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error translating chunk: {e}\")\n",
    "                    # Retry with smaller piece\n",
    "                    words = chunk.split()\n",
    "                    half = len(words) // 2\n",
    "                    try:\n",
    "                        part1 = self.translator.translate(\" \".join(words[:half]), \n",
    "                                                          dest=self.target_language).text\n",
    "                        part2 = self.translator.translate(\" \".join(words[half:]), \n",
    "                                                          dest=self.target_language).text\n",
    "                        translated_chunks.append(part1 + \" \" + part2)\n",
    "                    except:\n",
    "                        # If all else fails, add untranslated\n",
    "                        translated_chunks.append(chunk)\n",
    "            \n",
    "            translated_text = ' '.join(translated_chunks)\n",
    "            \n",
    "            # Save the translated text\n",
    "            with open(self.translated_text_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(translated_text)\n",
    "            \n",
    "            print(f\"Translation complete. Saved to {self.translated_text_path}\")\n",
    "            return translated_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in translation: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def generate_speech(self, text=None):\n",
    "        \"\"\"Generate better quality Hindi speech with improved pacing\"\"\"\n",
    "        if text is None:\n",
    "            try:\n",
    "                with open(self.translated_text_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Translation file not found at {self.translated_text_path}\")\n",
    "                return False\n",
    "        \n",
    "        print(f\"Generating improved speech in {self.language_names.get(self.target_language, self.target_language)}...\")\n",
    "        \n",
    "        try:\n",
    "            # Break text into smaller chunks for better TTS processing\n",
    "            # This improves voice quality and reduces potential errors\n",
    "            max_chunk_length = 500  # characters\n",
    "            text_chunks = []\n",
    "            \n",
    "            # Split by sentences where possible\n",
    "            sentences = text.replace('।', '.').split('.')\n",
    "            current_chunk = \"\"\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                if len(current_chunk) + len(sentence) < max_chunk_length:\n",
    "                    current_chunk += sentence + \". \"\n",
    "                else:\n",
    "                    if current_chunk:\n",
    "                        text_chunks.append(current_chunk)\n",
    "                    current_chunk = sentence + \". \"\n",
    "            \n",
    "            # Add the last chunk\n",
    "            if current_chunk:\n",
    "                text_chunks.append(current_chunk)\n",
    "            \n",
    "            # Generate speech for each chunk\n",
    "            audio_segments = []\n",
    "            \n",
    "            for i, chunk in enumerate(tqdm(text_chunks, desc=\"Generating speech chunks\")):\n",
    "                chunk_path = os.path.join(self.output_dir, f\"speech_chunk_{i}.mp3\")\n",
    "                \n",
    "                # Generate speech with improved settings\n",
    "                tts = gTTS(text=chunk, lang=self.target_language, slow=False)\n",
    "                tts.save(chunk_path)\n",
    "                \n",
    "                # Load as AudioSegment\n",
    "                segment = AudioSegment.from_mp3(chunk_path)\n",
    "                audio_segments.append(segment)\n",
    "                \n",
    "                # Clean up\n",
    "                os.remove(chunk_path)\n",
    "            \n",
    "            # Concatenate all segments\n",
    "            combined_audio = AudioSegment.empty()\n",
    "            for segment in audio_segments:\n",
    "                combined_audio += segment\n",
    "            \n",
    "            # Save combined audio\n",
    "            combined_audio.export(self.generated_audio_path, format=\"mp3\")\n",
    "            \n",
    "            # Convert to WAV for processing\n",
    "            audio = AudioSegment.from_mp3(self.generated_audio_path)\n",
    "            audio.export(self.processed_audio_path, format=\"wav\")\n",
    "            \n",
    "            print(f\"Speech generation complete. Saved to {self.generated_audio_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating speech: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def analyze_voice_characteristics(self):\n",
    "        \"\"\"Analyze the original voice characteristics for improved voice cloning\"\"\"\n",
    "        print(\"Analyzing original voice characteristics...\")\n",
    "        \n",
    "        try:\n",
    "            # Load audio\n",
    "            y, sr = librosa.load(self.extracted_audio_path, sr=None)\n",
    "            \n",
    "            if len(y) == 0:\n",
    "                print(\"Error: Empty audio file\")\n",
    "                return\n",
    "            \n",
    "            # Extract enhanced voice characteristics\n",
    "            # Pitch (fundamental frequency) with better filtering\n",
    "            pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "            # Filter out low magnitude components for better pitch estimation\n",
    "            magnitude_threshold = np.percentile(magnitudes, 75)  # Only top 25% magnitudes\n",
    "            pitches = pitches[magnitudes > magnitude_threshold]\n",
    "            \n",
    "            # Calculate pitch statistics (with outlier removal)\n",
    "            if len(pitches) > 0:\n",
    "                # Remove outliers (values outside 1.5 IQR)\n",
    "                q1, q3 = np.percentile(pitches, [25, 75])\n",
    "                iqr = q3 - q1\n",
    "                pitch_filtered = pitches[(pitches >= q1 - 1.5 * iqr) & (pitches <= q3 + 1.5 * iqr)]\n",
    "                \n",
    "                if len(pitch_filtered) > 0:\n",
    "                    pitch_mean = np.mean(pitch_filtered)\n",
    "                    pitch_std = np.std(pitch_filtered)\n",
    "                else:\n",
    "                    pitch_mean = np.mean(pitches)\n",
    "                    pitch_std = np.std(pitches)\n",
    "            else:\n",
    "                pitch_mean = 0\n",
    "                pitch_std = 0\n",
    "            \n",
    "            # Tempo with better beat tracking\n",
    "            onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "            tempo, _ = librosa.beat.beat_track(onset_envelope=onset_env, sr=sr)\n",
    "            \n",
    "            # Enhanced spectral features\n",
    "            spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
    "            spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n",
    "            spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr))\n",
    "            \n",
    "            # Formant estimation (approximation using MFCC)\n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "            mfcc_means = np.mean(mfccs, axis=1)\n",
    "            \n",
    "            # Store characteristics\n",
    "            self.voice_characteristics = {\n",
    "                'pitch_mean': float(pitch_mean),\n",
    "                'pitch_std': float(pitch_std),\n",
    "                'tempo': float(tempo),\n",
    "                'spectral_centroid': float(spectral_centroid),\n",
    "                'spectral_bandwidth': float(spectral_bandwidth),\n",
    "                'spectral_contrast': float(spectral_contrast),\n",
    "                'mfcc_profile': mfcc_means.tolist(),\n",
    "                'original_duration': self.original_duration,\n",
    "                'original_word_count': self.original_word_count\n",
    "            }\n",
    "            \n",
    "            print(\"Enhanced voice characteristics analysis complete\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in voice analysis: {e}\")\n",
    "    \n",
    "    def improve_audio_pacing(self):\n",
    "        \"\"\"Adjust the pacing of the generated audio to match the original\"\"\"\n",
    "        print(\"Improving audio pacing to match original video...\")\n",
    "        \n",
    "        try:\n",
    "            if not self.voice_characteristics:\n",
    "                print(\"Voice characteristics not available. Skipping pacing adjustment.\")\n",
    "                shutil.copy(self.processed_audio_path, self.final_audio_path)\n",
    "                return True\n",
    "            \n",
    "            # Load generated audio\n",
    "            y_generated, sr_generated = librosa.load(self.processed_audio_path, sr=None)\n",
    "            generated_duration = len(y_generated) / sr_generated\n",
    "            \n",
    "            # Calculate target duration based on original\n",
    "            original_duration = self.voice_characteristics['original_duration']\n",
    "            \n",
    "            # Determine if we need to stretch or compress\n",
    "            time_ratio = original_duration / generated_duration\n",
    "            \n",
    "            # Apply time stretching with phase vocoder for better quality\n",
    "            if 0.8 <= time_ratio <= 1.2:\n",
    "                # For small adjustments, use high quality time stretch\n",
    "                print(f\"Applying gentle pacing adjustment (ratio: {time_ratio:.2f})\")\n",
    "                y_adjusted = librosa.effects.time_stretch(y_generated, rate=time_ratio)\n",
    "            else:\n",
    "                # For larger adjustments, use more conservative approach\n",
    "                # This prevents extreme distortion\n",
    "                print(f\"Applying significant pacing adjustment (ratio: {time_ratio:.2f})\")\n",
    "                \n",
    "                # Cap maximum adjustment to prevent extreme distortion\n",
    "                capped_ratio = max(min(time_ratio, 1.5), 0.75)\n",
    "                y_adjusted = librosa.effects.time_stretch(y_generated, rate=capped_ratio)\n",
    "                \n",
    "                # If we had to cap significantly, add some silence at the end if needed\n",
    "                adjusted_duration = len(y_adjusted) / sr_generated\n",
    "                if adjusted_duration < original_duration - 1.0:  # If still more than 1 sec off\n",
    "                    silence_needed = int((original_duration - adjusted_duration) * sr_generated)\n",
    "                    y_adjusted = np.concatenate([y_adjusted, np.zeros(silence_needed)])\n",
    "            \n",
    "            # Save adjusted audio\n",
    "            sf.write(self.final_audio_path, y_adjusted, sr_generated)\n",
    "            print(f\"Audio pacing adjusted. Original: {original_duration:.2f}s, Generated: {generated_duration:.2f}s, Final: {len(y_adjusted)/sr_generated:.2f}s\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adjusting audio pacing: {e}\")\n",
    "            # Fallback to original processed audio\n",
    "            shutil.copy(self.processed_audio_path, self.final_audio_path)\n",
    "            return False\n",
    "    \n",
    "    def apply_voice_cloning(self):\n",
    "        \"\"\"Apply improved voice characteristic modifications\"\"\"\n",
    "        if not self.voice_characteristics:\n",
    "            print(\"Voice characteristics not available. Run analyze_voice_characteristics() first.\")\n",
    "            return False\n",
    "        \n",
    "        print(\"Applying enhanced voice cloning...\")\n",
    "        \n",
    "        try:\n",
    "            # Load generated speech\n",
    "            y_target, sr_target = librosa.load(self.processed_audio_path, sr=None)\n",
    "            \n",
    "            # IMPROVED: Create a copy for comparison\n",
    "            y_original = np.copy(y_target)\n",
    "            \n",
    "            # Pitch shifting based on mean pitch difference\n",
    "            # Use a more subtle pitch shift factor for more natural results\n",
    "            pitch_shift_factor = min(max(self.voice_characteristics['pitch_mean'] / 100, -2.5), 2.5)\n",
    "            print(f\"Applying pitch shift of {pitch_shift_factor:.2f} steps\")\n",
    "            \n",
    "            # Apply pitch shifting with improved formant preservation\n",
    "            y_processed = librosa.effects.pitch_shift(\n",
    "                y_target, \n",
    "                sr=sr_target, \n",
    "                n_steps=pitch_shift_factor\n",
    "            )\n",
    "            \n",
    "            # Apply subtle EQ based on spectral characteristics\n",
    "            # This simulates the frequency response pattern of the original voice\n",
    "            if self.voice_characteristics['spectral_centroid'] > 0:\n",
    "                # Create a simple filter based on spectral centroid difference\n",
    "                target_centroid = np.mean(librosa.feature.spectral_centroid(y=y_processed, sr=sr_target))\n",
    "                centroid_ratio = self.voice_characteristics['spectral_centroid'] / target_centroid\n",
    "                \n",
    "                # Apply a simple EQ boost/cut based on the centroid ratio\n",
    "                if 0.8 <= centroid_ratio <= 1.2:  # Only apply if the difference is reasonable\n",
    "                    if centroid_ratio < 1:  # Boost highs\n",
    "                        print(\"Applying subtle high frequency boost\")\n",
    "                        # Split signal into high and low components\n",
    "                        y_low = librosa.effects.preemphasis(y_processed, coef=-0.2)\n",
    "                        y_high = y_processed - y_low\n",
    "                        # Boost high frequencies\n",
    "                        boost_factor = 1 + (1 - centroid_ratio)\n",
    "                        y_processed = y_low + y_high * boost_factor\n",
    "                    else:  # Boost lows\n",
    "                        print(\"Applying subtle low frequency boost\")\n",
    "                        y_low = librosa.effects.preemphasis(y_processed, coef=-0.2)\n",
    "                        y_high = y_processed - y_low\n",
    "                        # Boost low frequencies\n",
    "                        boost_factor = centroid_ratio\n",
    "                        y_processed = y_low * boost_factor + y_high\n",
    "            \n",
    "            # Normalize audio levels\n",
    "            y_processed = librosa.util.normalize(y_processed)\n",
    "            \n",
    "            # Save processed audio\n",
    "            sf.write(self.processed_audio_path, y_processed, sr_target)\n",
    "            \n",
    "            print(\"Enhanced voice cloning applied\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in voice cloning: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def extract_video_frames(self):\n",
    "        \"\"\"Extract frames from the input video\"\"\"\n",
    "        print(\"Extracting video frames...\")\n",
    "        \n",
    "        video = cv2.VideoCapture(self.input_video_path)\n",
    "        success, frame = video.read()\n",
    "        \n",
    "        if not success:\n",
    "            print(\"Failed to read video\")\n",
    "            return False\n",
    "        \n",
    "        fps = video.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # Clear existing frames\n",
    "        for file in os.listdir(self.frames_dir):\n",
    "            if file.endswith('.jpg'):\n",
    "                os.remove(os.path.join(self.frames_dir, file))\n",
    "        \n",
    "        # Extract frames\n",
    "        count = 0\n",
    "        while success:\n",
    "            frame_path = os.path.join(self.frames_dir, f\"frame_{count:06d}.jpg\")\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            success, frame = video.read()\n",
    "            count += 1\n",
    "            if count % 100 == 0:\n",
    "                print(f\"Extracted {count}/{frame_count} frames\")\n",
    "        \n",
    "        video.release()\n",
    "        print(f\"Extracted {count} frames at {fps} FPS\")\n",
    "        return True\n",
    "    \n",
    "    def detect_mouth_landmarks(self, frame):\n",
    "        \"\"\"Detect mouth landmarks with improved accuracy\"\"\"\n",
    "        if self.predictor is None:\n",
    "            return None\n",
    "        \n",
    "        # Convert to grayscale for face detection\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Enhance contrast for better face detection\n",
    "        gray = cv2.equalizeHist(gray)\n",
    "        \n",
    "        # Try multiple face detection methods for better reliability\n",
    "        # Method 1: dlib's detector\n",
    "        faces_dlib = self.detector(gray)\n",
    "        \n",
    "        # Method 2: face_recognition library detector (HOG-based)\n",
    "        face_locations = face_recognition.face_locations(gray)\n",
    "        \n",
    "        # Consolidate detected faces\n",
    "        faces = []\n",
    "        \n",
    "        # Convert dlib rectangles to coordinates\n",
    "        for face in faces_dlib:\n",
    "            faces.append((face.top(), face.right(), face.bottom(), face.left()))\n",
    "        \n",
    "        # Add face_recognition detections\n",
    "        for top, right, bottom, left in face_locations:\n",
    "            # Check if this face is already in our list (avoid duplicates)\n",
    "            is_duplicate = False\n",
    "            for f_top, f_right, f_bottom, f_left in faces:\n",
    "                # If centers are close, consider it a duplicate\n",
    "                f_center_x = (f_left + f_right) // 2\n",
    "                f_center_y = (f_top + f_bottom) // 2\n",
    "                center_x = (left + right) // 2\n",
    "                center_y = (top + bottom) // 2\n",
    "                \n",
    "                dist = ((f_center_x - center_x)**2 + (f_center_y - center_y)**2)**0.5\n",
    "                if dist < 30:  # Threshold for duplicate detection\n",
    "                    is_duplicate = True\n",
    "                    break\n",
    "            \n",
    "            if not is_duplicate:\n",
    "                faces.append((top, right, bottom, left))\n",
    "        \n",
    "        if not faces:\n",
    "            return None\n",
    "        \n",
    "        # Find the largest face\n",
    "        largest_face = max(faces, key=lambda rect: (rect[2]-rect[0])*(rect[1]-rect[3]))\n",
    "        top, right, bottom, left = largest_face\n",
    "        \n",
    "        # Convert back to dlib rectangle for landmark prediction\n",
    "        rect = dlib.rectangle(left, top, right, bottom)\n",
    "        \n",
    "        # Get face landmarks with dlib\n",
    "        landmarks = self.predictor(gray, rect)\n",
    "        \n",
    "        # Extract mouth landmarks (points 48-68 in the 68-point model)\n",
    "        mouth_points = []\n",
    "        for i in range(48, 68):\n",
    "            x = landmarks.part(i).x\n",
    "            y = landmarks.part(i).y\n",
    "            mouth_points.append((x, y))\n",
    "        \n",
    "        return mouth_points\n",
    "    \n",
    "    def analyze_audio_for_phonemes(self, audio_path):\n",
    "        \"\"\"Analyze audio for improved lip sync\"\"\"\n",
    "        print(\"Analyzing audio for enhanced lip sync...\")\n",
    "        \n",
    "        try:\n",
    "            # Load audio with higher precision\n",
    "            y, sr = librosa.load(audio_path, sr=None)\n",
    "            \n",
    "            # Compute enhanced onset strength (sensitive to sudden changes)\n",
    "            onset_env = librosa.onset.onset_strength(y=y, sr=sr, \n",
    "                                                     hop_length=512, \n",
    "                                                     aggregate=np.median)\n",
    "            \n",
    "            # Detect onsets with adaptive thresholding\n",
    "            onsets = librosa.onset.onset_detect(onset_envelope=onset_env, \n",
    "                                                sr=sr,\n",
    "                                                wait=1,  # Wait between consecutive onsets\n",
    "                                                pre_avg=1,  # Previous frames for comparison\n",
    "                                                post_avg=1,  # Future frames for comparison\n",
    "                                                pre_max=1,  # Previous frames for max\n",
    "                                                post_max=1)  # Future frames for max\n",
    "            \n",
    "            onset_times = librosa.frames_to_time(onsets, sr=sr)\n",
    "            \n",
    "            # Convert to frame indices based on video FPS\n",
    "            video = cv2.VideoCapture(self.input_video_path)\n",
    "            fps = video.get(cv2.CAP_PROP_FPS)\n",
    "            video.release()\n",
    "            \n",
    "            onset_frames = [int(time * fps) for time in onset_times]\n",
    "            \n",
    "            # Calculate RMS energy for more accurate lip opening amount\n",
    "            hop_length = 512\n",
    "            frame_length = 2048\n",
    "            \n",
    "            # Get RMS energy (better than amplitude for mouth movement)\n",
    "            rms_energy = librosa.feature.rms(y=y, \n",
    "                                          frame_length=frame_length, \n",
    "                                          hop_length=hop_length)[0]\n",
    "            \n",
    "            # Smooth the energy curve for more natural transitions\n",
    "            rms_energy = ndi.gaussian_filter1d(rms_energy, sigma=2)\n",
    "            \n",
    "            # Convert to frame indices\n",
    "            audio_frames = librosa.frames_to_time(np.arange(len(rms_energy)), \n",
    "                                            sr=sr, \n",
    "                                            hop_length=hop_length)\n",
    "            \n",
    "            # Interpolate to match video frame rate\n",
    "            total_video_frames = len([f for f in os.listdir(self.frames_dir) if f.endswith('.jpg')])\n",
    "            video_duration = total_video_frames / fps\n",
    "            \n",
    "            # Create enhanced interpolation function\n",
    "            interp_func = interp1d(\n",
    "                audio_frames,\n",
    "                rms_energy,\n",
    "                kind='cubic',  # Cubic interpolation for smoother transitions\n",
    "                bounds_error=False,\n",
    "                fill_value=(rms_energy[0], rms_energy[-1])\n",
    "            )\n",
    "            \n",
    "            # Sample at video frame rate\n",
    "            video_frame_times = np.arange(0, video_duration, 1/fps)\n",
    "            video_frame_times = video_frame_times[:total_video_frames]\n",
    "            lip_openness = interp_func(video_frame_times)\n",
    "            \n",
    "            # Normalize between 0 and 1\n",
    "            lip_openness = (lip_openness - np.min(lip_openness)) / (np.max(lip_openness) - np.min(lip_openness))\n",
    "            \n",
    "            # Apply additional smoothing for natural transitions\n",
    "            lip_openness = ndi.gaussian_filter1d(lip_openness, sigma=1.5)\n",
    "            \n",
    "            return onset_frames, lip_openness\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing audio for lip sync: {e}\")\n",
    "            return [], []\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e4b9eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--output_dir OUTPUT_DIR]\n",
      "                             [--language LANGUAGE] [--keep_temp]\n",
      "                             input_video\n",
      "ipykernel_launcher.py: error: the following arguments are required: input_video\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "def apply_lip_sync(self):\n",
    "    \"\"\"Apply improved lip sync with better mouth shape modeling\"\"\"\n",
    "    print(\"Applying enhanced lip sync...\")\n",
    "    \n",
    "    # Analyze the generated audio for phoneme timing\n",
    "    onset_frames, lip_openness = self.analyze_audio_for_phonemes(self.final_audio_path)\n",
    "    \n",
    "    if not onset_frames and len(lip_openness) == 0:\n",
    "        print(\"Failed to analyze audio for lip sync\")\n",
    "        return False\n",
    "    \n",
    "    # Process each frame\n",
    "    frame_files = sorted([f for f in os.listdir(self.frames_dir) if f.endswith('.jpg')])\n",
    "    \n",
    "    processed_dir = os.path.join(self.output_dir, \"processed_frames\")\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "    \n",
    "    # Track mouth positions across frames for consistency\n",
    "    mouth_history = []  # Store last several mouth points\n",
    "    history_size = 5    # How many frames to consider for smoothing\n",
    "    \n",
    "    for i, frame_file in enumerate(tqdm(frame_files, desc=\"Processing frames for lip sync\")):\n",
    "        frame_path = os.path.join(self.frames_dir, frame_file)\n",
    "        frame = cv2.imread(frame_path)\n",
    "        \n",
    "        if frame is None:\n",
    "            print(f\"Error reading frame {frame_file}\")\n",
    "            continue\n",
    "            \n",
    "        # Try to detect mouth in current frame\n",
    "        mouth_points = self.detect_mouth_landmarks(frame)\n",
    "        \n",
    "        # If detection failed, try using history or skip\n",
    "        if mouth_points is None:\n",
    "            if len(mouth_history) > 0:\n",
    "                # Use average of recent mouth positions\n",
    "                mouth_points = np.mean(mouth_history, axis=0).astype(int).tolist()\n",
    "            else:\n",
    "                # If no history and no detection, just copy the frame\n",
    "                output_path = os.path.join(processed_dir, frame_file)\n",
    "                cv2.imwrite(output_path, frame)\n",
    "                continue\n",
    "        else:\n",
    "            # Update history with new detection\n",
    "            mouth_history.append(mouth_points)\n",
    "            if len(mouth_history) > history_size:\n",
    "                mouth_history.pop(0)\n",
    "        \n",
    "        # Calculate mouth center and average distance from center\n",
    "        mouth_center_x = sum(p[0] for p in mouth_points) // len(mouth_points)\n",
    "        mouth_center_y = sum(p[1] for p in mouth_points) // len(mouth_points)\n",
    "        \n",
    "        # Determine if this is a frame where mouth should move\n",
    "        # Check if this frame is near an onset or has high energy\n",
    "        is_onset = False\n",
    "        for onset in onset_frames:\n",
    "            if abs(i - onset) < 3:  # Within 3 frames of an onset\n",
    "                is_onset = True\n",
    "                break\n",
    "        \n",
    "        # Get lip openness factor for this frame (if available)\n",
    "        lip_factor = 0.5  # Default middle value\n",
    "        if i < len(lip_openness):\n",
    "            lip_factor = lip_openness[i]\n",
    "        \n",
    "        # Apply morphing based on lip movement required\n",
    "        modified_frame = frame.copy()\n",
    "        \n",
    "        if mouth_points:\n",
    "            # Create mask for the mouth region\n",
    "            mask = np.zeros_like(frame)\n",
    "            hull = cv2.convexHull(np.array(mouth_points))\n",
    "            cv2.fillConvexPoly(mask, hull, (255, 255, 255))\n",
    "            \n",
    "            # Extract inner and outer lip points\n",
    "            inner_lips = mouth_points[12:20]  # Inner lip points (approximate)\n",
    "            outer_lips = mouth_points[0:12]   # Outer lip points (approximate)\n",
    "\n",
    "            # Adjust mouth shape based on lip factor and audio onset\n",
    "            modified_points = []\n",
    "            \n",
    "            for p in mouth_points:\n",
    "                x, y = p\n",
    "                dx = x - mouth_center_x\n",
    "                dy = y - mouth_center_y\n",
    "                \n",
    "                # Determine if this is an inner lip point (closer to center)\n",
    "                dist_from_center = ((x - mouth_center_x)**2 + (y - mouth_center_y)**2)**0.5\n",
    "                is_inner_point = dist_from_center < np.mean([((p[0] - mouth_center_x)**2 + \n",
    "                                                            (p[1] - mouth_center_y)**2)**0.5 \n",
    "                                                            for p in mouth_points])\n",
    "                \n",
    "                # Apply different transformations to inner vs outer lips\n",
    "                if is_inner_point:\n",
    "                    # Inner lips open more dramatically\n",
    "                    factor = 1.0 + (lip_factor * 0.7)  # More opening for inner lips\n",
    "                    if is_onset:\n",
    "                        factor += 0.3  # Extra opening on audio onsets\n",
    "                else:\n",
    "                    # Outer lips move less\n",
    "                    factor = 1.0 + (lip_factor * 0.3)  # Less movement for outer lips\n",
    "                    if is_onset:\n",
    "                        factor += 0.1  # Smaller extra movement on audio onsets\n",
    "                \n",
    "                # Only stretch vertically for more natural mouth movement\n",
    "                if dy > 0:  # Lower lip points move down\n",
    "                    new_y = int(mouth_center_y + dy * factor)\n",
    "                    modified_points.append((x, new_y))\n",
    "                elif dy < 0:  # Upper lip points move up\n",
    "                    new_y = int(mouth_center_y + dy * factor)\n",
    "                    modified_points.append((x, new_y))\n",
    "                else:  # Points at center height stay the same\n",
    "                    modified_points.append((x, y))\n",
    "            \n",
    "            # Smoothly warp the mouth region\n",
    "            if len(modified_points) == len(mouth_points):\n",
    "                # Create triangulation for warping\n",
    "                rect = (0, 0, frame.shape[1], frame.shape[0])\n",
    "                subdiv = cv2.Subdiv2D(rect)\n",
    "                \n",
    "                # Add points around the image and mouth\n",
    "                # This ensures the warp is contained to the mouth area\n",
    "                for x, y in [(0, 0), (frame.shape[1] - 1, 0), \n",
    "                             (0, frame.shape[0] - 1), (frame.shape[1] - 1, frame.shape[0] - 1)]:\n",
    "                    subdiv.insert((x, y))\n",
    "                \n",
    "                # Add points around the mouth region to limit warping effect\n",
    "                mouth_rect = cv2.boundingRect(np.array(mouth_points))\n",
    "                x, y, w, h = mouth_rect\n",
    "                padding = 30  # Add padding around mouth\n",
    "                \n",
    "                for px, py in [(x-padding, y-padding), (x+w+padding, y-padding), \n",
    "                             (x-padding, y+h+padding), (x+w+padding, y+h+padding)]:\n",
    "                    if 0 <= px < frame.shape[1] and 0 <= py < frame.shape[0]:\n",
    "                        subdiv.insert((px, py))\n",
    "                \n",
    "                # Add original mouth points\n",
    "                for i, (x, y) in enumerate(mouth_points):\n",
    "                    subdiv.insert((x, y))\n",
    "                \n",
    "                # Simple blend for smoother transition\n",
    "                alpha = 0.8  # Blend factor\n",
    "                modified_frame = cv2.addWeighted(frame, 1-alpha, modified_frame, alpha, 0)\n",
    "        \n",
    "        # Save the processed frame\n",
    "        output_path = os.path.join(processed_dir, frame_file)\n",
    "        cv2.imwrite(output_path, modified_frame)\n",
    "    \n",
    "    print(\"Lip sync processing complete\")\n",
    "    return True\n",
    "\n",
    "def combine_audio_video(self):\n",
    "    \"\"\"Combine the processed frames with the dubbed audio\"\"\"\n",
    "    print(\"Combining audio and video...\")\n",
    "    \n",
    "    processed_frames_dir = os.path.join(self.output_dir, \"processed_frames\")\n",
    "    \n",
    "    # Check if processed frames exist\n",
    "    if not os.path.exists(processed_frames_dir) or len(os.listdir(processed_frames_dir)) == 0:\n",
    "        print(\"No processed frames found, using original frames\")\n",
    "        processed_frames_dir = self.frames_dir\n",
    "    \n",
    "    # Get video properties\n",
    "    video = cv2.VideoCapture(self.input_video_path)\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    video.release()\n",
    "    \n",
    "    # Create video from frames\n",
    "    frame_files = sorted([f for f in os.listdir(processed_frames_dir) if f.endswith('.jpg')])\n",
    "    \n",
    "    # Create a temporary video file without audio\n",
    "    temp_video_path = os.path.join(self.output_dir, \"temp_video.mp4\")\n",
    "    \n",
    "    # Use OpenCV VideoWriter for better control\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(temp_video_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    for frame_file in tqdm(frame_files, desc=\"Combining frames into video\"):\n",
    "        frame_path = os.path.join(processed_frames_dir, frame_file)\n",
    "        frame = cv2.imread(frame_path)\n",
    "        if frame is not None:\n",
    "            out.write(frame)\n",
    "    \n",
    "    out.release()\n",
    "    \n",
    "    # Combine video with dubbed audio using ffmpeg (more reliable)\n",
    "    try:\n",
    "        # Ensure ffmpeg is available\n",
    "        subprocess.run(['ffmpeg', '-version'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)\n",
    "        \n",
    "        # Run ffmpeg command\n",
    "        cmd = [\n",
    "            'ffmpeg',\n",
    "            '-y',  # Overwrite output file if it exists\n",
    "            '-i', temp_video_path,  # Input video\n",
    "            '-i', self.final_audio_path,  # Input audio\n",
    "            '-c:v', 'copy',  # Copy video stream without re-encoding\n",
    "            '-c:a', 'aac',  # Use AAC audio codec\n",
    "            '-strict', 'experimental',\n",
    "            '-map', '0:v:0',  # Map first video stream from first input\n",
    "            '-map', '1:a:0',  # Map first audio stream from second input\n",
    "            '-shortest',  # Finish encoding when the shortest input stream ends\n",
    "            self.final_output_path\n",
    "        ]\n",
    "        \n",
    "        subprocess.run(cmd, check=True)\n",
    "        print(f\"Video and audio combined successfully: {self.final_output_path}\")\n",
    "        \n",
    "        # Clean up temporary file\n",
    "        os.remove(temp_video_path)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error combining video and audio: {e}\")\n",
    "        \n",
    "        # Fallback to moviepy if ffmpeg fails\n",
    "        try:\n",
    "            print(\"Trying alternative method with moviepy...\")\n",
    "            video_clip = mp.VideoFileClip(temp_video_path)\n",
    "            audio_clip = mp.AudioFileClip(self.final_audio_path)\n",
    "            \n",
    "            final_clip = video_clip.set_audio(audio_clip)\n",
    "            final_clip.write_videofile(self.final_output_path, codec='libx264', audio_codec='aac')\n",
    "            \n",
    "            # Clean up\n",
    "            video_clip.close()\n",
    "            audio_clip.close()\n",
    "            os.remove(temp_video_path)\n",
    "            \n",
    "            print(f\"Video and audio combined successfully: {self.final_output_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as inner_e:\n",
    "            print(f\"Error in fallback method: {inner_e}\")\n",
    "            return False\n",
    "\n",
    "def process_video(self):\n",
    "    \"\"\"Process the entire video dubbing pipeline\"\"\"\n",
    "    print(f\"Starting video dubbing process for {os.path.basename(self.input_video_path)}...\")\n",
    "    print(f\"Target language: {self.language_names.get(self.target_language, self.target_language)}\")\n",
    "    \n",
    "    # Step 1: Extract audio\n",
    "    if not self.extract_audio():\n",
    "        print(\"Failed to extract audio. Aborting.\")\n",
    "        return False\n",
    "    \n",
    "    # Step 2: Transcribe audio\n",
    "    transcribed_text = self.transcribe_audio()\n",
    "    if not transcribed_text:\n",
    "        print(\"Failed to transcribe audio. Aborting.\")\n",
    "        return False\n",
    "    \n",
    "    # Step 3: Translate text\n",
    "    translated_text = self.translate_text(transcribed_text)\n",
    "    if not translated_text:\n",
    "        print(\"Failed to translate text. Aborting.\")\n",
    "        return False\n",
    "    \n",
    "    # Step 4: Generate speech\n",
    "    if not self.generate_speech(translated_text):\n",
    "        print(\"Failed to generate speech. Aborting.\")\n",
    "        return False\n",
    "    \n",
    "    # Step 5: Analyze voice characteristics\n",
    "    self.analyze_voice_characteristics()\n",
    "    \n",
    "    # Step 6: Apply voice cloning\n",
    "    self.apply_voice_cloning()\n",
    "    \n",
    "    # Step 7: Improve audio pacing\n",
    "    self.improve_audio_pacing()\n",
    "    \n",
    "    # Step 8: Extract video frames\n",
    "    if not self.extract_video_frames():\n",
    "        print(\"Failed to extract video frames. Skipping lip sync.\")\n",
    "        return self.combine_audio_video()\n",
    "    \n",
    "    # Step 9: Apply lip sync\n",
    "    self.apply_lip_sync()\n",
    "    \n",
    "    # Step 10: Combine audio and video\n",
    "    if not self.combine_audio_video():\n",
    "        print(\"Failed to combine audio and video.\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"Video dubbing complete! Output saved to {self.final_output_path}\")\n",
    "    return True\n",
    "\n",
    "def clean_up(self):\n",
    "    \"\"\"Clean up temporary files\"\"\"\n",
    "    print(\"Cleaning up temporary files...\")\n",
    "    \n",
    "    temp_files = [\n",
    "        self.extracted_audio_path,\n",
    "        self.processed_audio_path,\n",
    "        self.generated_audio_path\n",
    "    ]\n",
    "    \n",
    "    temp_dirs = [\n",
    "        self.frames_dir,\n",
    "        os.path.join(self.output_dir, \"processed_frames\")\n",
    "    ]\n",
    "    \n",
    "    # Remove temp files\n",
    "    for file_path in temp_files:\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error removing {file_path}: {e}\")\n",
    "    \n",
    "    # Remove temp directories\n",
    "    for dir_path in temp_dirs:\n",
    "        if os.path.exists(dir_path):\n",
    "            try:\n",
    "                shutil.rmtree(dir_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error removing directory {dir_path}: {e}\")\n",
    "    \n",
    "    print(\"Cleanup complete\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Video Dubbing Tool')\n",
    "    parser.add_argument('input_video', help=r\"C:\\Users\\Admin\\Documents\\AI_Project(2.0)\\Input_video.mp4\")\n",
    "    parser.add_argument('--output_dir', '-o', default='output', help=r\"C:\\Users\\Admin\\Documents\\AI_Project(2.0)\\output\")\n",
    "    parser.add_argument('--language', '-l', default='hi', help='Target language code (e.g., hi for Hindi)')\n",
    "    parser.add_argument('--keep_temp', '-k', action='store_true', help='Keep temporary files after processing')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Create dubber instance\n",
    "    dubber = ImprovedVideoDubber(\n",
    "        input_video_path=args.input_video,\n",
    "        output_dir=args.output_dir,\n",
    "        target_language=args.language\n",
    "    )\n",
    "    \n",
    "    # Process video\n",
    "    success = dubber.process_video()\n",
    "    \n",
    "    # Clean up temp files if requested\n",
    "    if success and not args.keep_temp:\n",
    "        dubber.clean_up()\n",
    "    \n",
    "    if success:\n",
    "        print(f\"Dubbing completed successfully! Output: {dubber.final_output_path}\")\n",
    "    else:\n",
    "        print(\"Dubbing process failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0afd0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lilly_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
